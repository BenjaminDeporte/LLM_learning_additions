{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80517dbc",
   "metadata": {
    "id": "80517dbc"
   },
   "source": [
    "# Teach an LLM to do additions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaca18f",
   "metadata": {},
   "source": [
    "The goal of this project is to teach an LLM to do additions, playing only with two parts:\n",
    "* the tokenizer\n",
    "* the positional embedding\n",
    "\n",
    "Both the model and the dataset are fixed.\n",
    "\n",
    "You are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of your tokenizer and positional embedding, you may change the number of bits. The initial value of 3 is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "id": "OzGh9ahKF17h"
   },
   "outputs": [],
   "source": [
    "number_bits = 4\n",
    "\n",
    "dataset_size = 64_000\n",
    "train_proportion = 0.9\n",
    "\n",
    "log_interval = 200\n",
    "batch_size = 64\n",
    "# learning_rate = 8e-4\n",
    "learning_rate = 1e-3\n",
    "\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "id": "t6aC9uNeIR6C"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMvT0B-MGBnY",
   "metadata": {
    "id": "BMvT0B-MGBnY"
   },
   "source": [
    "### Baseline: character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "id": "QuCc6jF5F8hK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8FXW2K-1Jd-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FXW2K-1Jd-P",
    "outputId": "349a4033-9fce-462b-f0d5-1bb3a7ffd340"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 10, 4, 2, 11], '12+42=')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"12 + 42 =\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3gckvebGGYt",
   "metadata": {
    "id": "j3gckvebGGYt"
   },
   "source": [
    "# Implement your tokenizer here!\n",
    "\n",
    "You can do anything (as long as you do not compute the addition!).\n",
    "Some ideas:\n",
    "* reversing numbers left to right\n",
    "* arranging by groups (of, 2, 3,...)\n",
    "* aligning numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec36c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTentativeTokenizer:\n",
    "    \"\"\"OK, this is my attempt to make a tokenizer.\n",
    "    The idea is simple :\n",
    "    1- clean up : remove all characters that are not numeric, nor '+' nor '='\n",
    "    2- assuming the prompts are \"a + b =\" : \n",
    "        2.1- split the prompt to get two lists of characters corresponding to a and b\n",
    "        2.2- work backward, ie from the rightmost character to the leftmost character in a and b\n",
    "        2.3- if one of the character is None (end of list), replace by [PAD]\n",
    "        2.4- form tuples (character 1, character 2). This is a token.\n",
    "    3- return all tokens\n",
    "    As a result, the vocabulary is the set of tuples (char 1 x char 2) where char 1 and 2 are in [0-9, +, =, [PAD]]. Plus [EOS] special token.\n",
    "    \"\"\"\n",
    "    \n",
    "    pad_token=\"[PAD]\"\n",
    "    eos_token=\"[EOS]\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unit_token_list = [str(x) for x in range(10)] + [\"+\", \"=\"] + [self.pad_token, self.eos_token]\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.unit_token_list))}]\"\n",
    "        self.vocab = [ (c1, c2) for c1 in self.unit_token_list for c2 in self.unit_token_list ] # not all tokens will ever be used, but never mind\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the unit character list\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            text (_type_): prompt as an input. Assumes to be \"a + b = \" where a and b are strings with integers.\n",
    "\n",
    "        Returns:\n",
    "            tokens (list): list of tokens, each token being an integer\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove all characters not in the vocabulary\n",
    "        cleaned_text = self.clean(text)\n",
    "        # split the prompt in two operands, remove everything else\n",
    "        pattern = r'\\+'\n",
    "        operands = re.split(pattern, cleaned_text)\n",
    "        # streamline the two operands\n",
    "        pattern = r'[ =]'\n",
    "        cleaned_operands = []\n",
    "        for op in operands:\n",
    "            c_op = re.sub(pattern, '', op)\n",
    "            cleaned_operands.append(c_op)\n",
    "        # build two lists of characters out of the cleaned operands\n",
    "        list1 = [c for c in cleaned_operands[0]]\n",
    "        list2 = [c for c in cleaned_operands[1]]\n",
    "        # work backward, egalize lengths and form tokens\n",
    "        tokens = []\n",
    "        list1.reverse()\n",
    "        list2.reverse()\n",
    "        if len(list1) < len(list2):\n",
    "            list1 = list1 + [self.pad_token] * (len(list2) - len(list1))\n",
    "        if len(list2) < len(list1):\n",
    "            list2 = list2 + [self.pad_token] * (len(list1) - len(list2))\n",
    "        # print(f\"list1 = {list1}\")\n",
    "        # print(f\"list2 = {list2}\")\n",
    "        for c1, c2 in zip(list1, list2):\n",
    "            pair = (c1, c2)\n",
    "            # print(pair)\n",
    "            tokens.append(self.token_to_id[pair])\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "    def decode(self, token_list):\n",
    "        \"\"\"Take the token list, find the associated pairs of characters, reverse and concatenate them to form the prompt\n",
    "\n",
    "        Args:\n",
    "            token_list (_type_): list of integers\n",
    "\n",
    "        Returns:\n",
    "            string : reconstructed prompt\n",
    "        \"\"\"\n",
    "        \n",
    "        list1 = []\n",
    "        list2 = []\n",
    "        # decompose the token list, find the associated pairs, create the two lists\n",
    "        for token in token_list:\n",
    "            pair = self.id_to_token[token]\n",
    "            c1 = pair[0]\n",
    "            c2 = pair[1]\n",
    "            if c1 != self.pad_token:\n",
    "                list1.append(c1)\n",
    "            if c2 != self.pad_token:\n",
    "                list2.append(c2)\n",
    "        # reverse and form the prompt\n",
    "        list1.reverse()\n",
    "        list2.reverse()\n",
    "        prompt = \"\".join(list1) + \" + \" + \"\".join(list2) + \" = \"\n",
    "            \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be232e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original prompt : '1245 + 447582 ='\n",
      "encoded prompt : [72, 64, 33, 21, 172, 172]\n",
      "decoded token list : '1245 + 447582 = '\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "\n",
    "tok = TheTentativeTokenizer()\n",
    "\n",
    "prompt = \"1245 + 447582 =\"\n",
    "print(f\"original prompt : '{prompt}'\")\n",
    "\n",
    "token_list = tok.encode(prompt)\n",
    "print(f\"encoded prompt : {token_list}\")\n",
    "\n",
    "dec = tok.decode(token_list=token_list)\n",
    "print(f\"decoded token list : '{dec}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daa90f31",
    "outputId": "3e8719ee-d8fa-4984-8b51-4db3457f7dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7947+1971=', '9918')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(number_bits = 3):\n",
    "    \"\"\"\n",
    "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
    "    \"\"\"\n",
    "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    sum_int = a_int + b_int\n",
    "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(number_bits=number_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6e861d2",
    "outputId": "c88c2226-0546-473c-c296-88a52823886b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5879+8922=', '14801'),\n",
       " ('3207+5014=', '8221'),\n",
       " ('7564+1335=', '8899'),\n",
       " ('2316+3542=', '5858')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fee85050",
    "outputId": "f080f4b0-fd76-48d8-d59f-7c118b6e6fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57600, 6400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7d2eb",
   "metadata": {},
   "source": [
    "### Baseline: the classical Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91674239",
   "metadata": {
    "id": "91674239"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # print(f\"position : {position.size()}\")\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # print(f\"pe = {pe}\")\n",
    "        # print(f\"pe = {pe.size()}\")\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ceb2",
   "metadata": {},
   "source": [
    "# Implement your positional embedding here!\n",
    "\n",
    "You can do anything. Some ideas:\n",
    "* RoPE\n",
    "* (randomised) FIRE\n",
    "* Abacus\n",
    "\n",
    "**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n",
    "(length_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d4fe0",
   "metadata": {},
   "source": [
    "### Abacus embedding seems straightforward to implement with good results : https://arxiv.org/html/2405.17399v1#S3.F2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59795a4e",
   "metadata": {},
   "source": [
    "###Â Abacus \n",
    "\n",
    "https://arxiv.org/html/2405.17399v1#S3.F2\n",
    "\n",
    "Quote :\n",
    "\n",
    "\"To address the limitations of transformers at representing positional information, we design a specially built positional embedding that encodes the location of each digit relative to the start of the current number. We call this Abacus Embeddings. We apply the same positional embedding to all digits of the same significance, providing an explicit signal that the model can use to align digits. \n",
    "\n",
    "We take inspiration from Randomized Embeddings (Ruoss et al., 2023) but instead of using random ascending indices to represent positions in a sample, we use consecutive ascending indices with a random starting position to allow for length generalization. Specifically, during training we give consecutive positional embeddings to each digit in a number, starting from a randomly chosen offset value from $U \\in [1,k]$, where k is a hyperparameter. Unless otherwise stated the default value for k in this study is 100. \n",
    "\n",
    "For example, if the input is 123, the positional encodings are $\\beta, \\beta+1, \\beta+2$, where $\\beta \\sim U \\in [1,100]$ which are then passed through a learned embedding matrix. The value sampled from $U \\in [1,k]$ is the same for all numbers in a batch, meaning all digits of the same significance obtain the same positional embedding. This training scheme allows the model to see a wide range of positional embeddings, even when training sequences are short. At test time, each positional embedding begins from one, i.e. $\\beta = 1$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd8b95",
   "metadata": {},
   "source": [
    "### Some thoughts on positional encoding\n",
    "\n",
    "In our case, the numbers of same significance (ie at the same position in the operands) are already grouped by the tokenizer.\n",
    "\n",
    "So the position of the token itself, should be representative of the relative positions of each digit.\n",
    "\n",
    "Should the vanilla positional embedding be enough ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "589793fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TheTentativePositionalEmbedding(nn.Module):\n",
    "#         def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "#                 super(PositionalEmbedding, self).__init__()\n",
    "#                 self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         # pe = torch.zeros(max_len, d_model)\n",
    "#         # position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         # print(f\"position : {position.size()}\")\n",
    "#         # div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         # pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         # pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         # pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         # print(f\"pe = {pe}\")\n",
    "#         # print(f\"pe = {pe.size()}\")\n",
    "#         # self.register_buffer('pe', pe)\n",
    "        \n",
    "#         def forward(self, x):\n",
    "#                 r\"\"\"Inputs of forward function\n",
    "#                 Args:\n",
    "#                 x: the sequence fed to the positional encoder model (required).\n",
    "#                 Shape:\n",
    "#                 x: [sequence length, batch size, embed dim]\n",
    "#                 output: [sequence length, batch size, embed dim]\n",
    "#                 \"\"\"\n",
    "#                 pass\n",
    "\n",
    "#         # x = x + self.pe[:x.size(0), :]\n",
    "#         # return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb278ab",
   "metadata": {
    "id": "4eb278ab"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp,\n",
    "                                               nhead=nhead,\n",
    "                                               dim_feedforward=nhid,\n",
    "                                               num_encoder_layers=nlayers)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEmbedding(ninp, dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output_enc = self.encoder(src, mask=self.src_mask)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return F.log_softmax(output_dec, dim=-1), output_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f9d1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e093a",
   "metadata": {},
   "source": [
    "Please do not change these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "f7f78975-2bdf-4c36-de35-3e140636d476"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin.deporte/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (input_emb): Embedding(14, 128)\n",
       "  (pos_encoder): PositionalEmbedding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f2f06e0",
   "metadata": {
    "id": "8f2f06e0"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5):\n",
    "    input_tensor = prompts # (length_prompts, batch_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (length_prompts, batch_size, ntokens)\n",
    "        last_output = output[-1,:,:] # (batch_size, ntokens)\n",
    "        token = torch.argmax(last_output, -1).view((1,-1)) # (1, batch_size)\n",
    "        input_tensor = torch.cat((input_tensor, token), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d76d1b19",
    "outputId": "a1df1dc9-2ecc-4de4-85b2-6bc5bd460439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 10,  3, 11, 13, 13, 13, 13, 13]], device='cuda:0'),\n",
       " '2+3=[EOS][EOS][EOS][EOS][EOS]')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00954ddc",
   "metadata": {
    "id": "00954ddc"
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c84beab",
    "outputId": "fc1bea13-d6e1-4a55-b70d-36de00bcec9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "264f9227",
   "metadata": {
    "id": "264f9227"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
    "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
    "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
    "    padded_answers, length_answers = pad(answers, \"answers\")\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91e281ad",
    "outputId": "22e2d0ee-ede4-41f8-e089-fb63ac2d9787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 64]), torch.Size([6, 64]), 10, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
    "X.shape, Y.shape, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cfcd10a",
   "metadata": {
    "id": "1cfcd10a"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n",
    "            prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n",
    "            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n",
    "            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac335b05",
    "outputId": "b475e943-51b3-401d-d18b-c9d32a49ffb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3638a75d",
   "metadata": {
    "id": "3638a75d"
   },
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
    "        prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "        target_answers = target_answers.to(device) # (length_answers, batch_size)\n",
    "        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n",
    "        model.zero_grad()\n",
    "        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n",
    "        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n",
    "        target_answers = target_answers.view(-1)\n",
    "        loss = F.cross_entropy(output_answers, target_answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def train(epochs=3):\n",
    "    accuracy_v_epoch = []\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch()\n",
    "        test_accuracy = evaluate()\n",
    "        accuracy_v_epoch.append(test_accuracy)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy\n",
    "            \n",
    "    return accuracy_v_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "143b3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 100 epochs on cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    epochs = 100\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    epochs = 4\n",
    "    \n",
    "print(f\"Training {epochs} epochs on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e2a8490",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e2a8490",
    "outputId": "f70dcac2-5891-4266-8748-85df050f4881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.71 | loss  1.90 | perplexity     6.68\n",
      "|   400/  900 batches | ms/batch 12.36 | loss  1.59 | perplexity     4.91\n",
      "|   600/  900 batches | ms/batch 12.38 | loss  1.49 | perplexity     4.42\n",
      "|   800/  900 batches | ms/batch 12.26 | loss  1.42 | perplexity     4.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.35s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.33 | loss  1.38 | perplexity     3.98\n",
      "|   400/  900 batches | ms/batch 12.35 | loss  1.35 | perplexity     3.85\n",
      "|   600/  900 batches | ms/batch 12.32 | loss  1.33 | perplexity     3.77\n",
      "|   800/  900 batches | ms/batch 12.59 | loss  1.33 | perplexity     3.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.87s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.62 | loss  1.32 | perplexity     3.73\n",
      "|   400/  900 batches | ms/batch 12.47 | loss  1.30 | perplexity     3.66\n",
      "|   600/  900 batches | ms/batch 12.29 | loss  1.29 | perplexity     3.63\n",
      "|   800/  900 batches | ms/batch 12.40 | loss  1.29 | perplexity     3.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.88s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.18 | loss  1.29 | perplexity     3.62\n",
      "|   400/  900 batches | ms/batch 12.23 | loss  1.27 | perplexity     3.57\n",
      "|   600/  900 batches | ms/batch 12.39 | loss  1.27 | perplexity     3.55\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  1.27 | perplexity     3.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.75s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.44 | loss  1.27 | perplexity     3.55\n",
      "|   400/  900 batches | ms/batch 12.30 | loss  1.25 | perplexity     3.50\n",
      "|   600/  900 batches | ms/batch 12.32 | loss  1.25 | perplexity     3.50\n",
      "|   800/  900 batches | ms/batch 12.33 | loss  1.26 | perplexity     3.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.81s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.73 | loss  1.26 | perplexity     3.51\n",
      "|   400/  900 batches | ms/batch 11.97 | loss  1.25 | perplexity     3.50\n",
      "|   600/  900 batches | ms/batch 11.94 | loss  1.24 | perplexity     3.45\n",
      "|   800/  900 batches | ms/batch 12.04 | loss  1.24 | perplexity     3.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.45s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.17 | loss  1.24 | perplexity     3.47\n",
      "|   400/  900 batches | ms/batch 11.99 | loss  1.23 | perplexity     3.44\n",
      "|   600/  900 batches | ms/batch 11.98 | loss  1.24 | perplexity     3.45\n",
      "|   800/  900 batches | ms/batch 12.09 | loss  1.23 | perplexity     3.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.55s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.15 | loss  1.23 | perplexity     3.44\n",
      "|   400/  900 batches | ms/batch 12.20 | loss  1.23 | perplexity     3.41\n",
      "|   600/  900 batches | ms/batch 12.13 | loss  1.23 | perplexity     3.41\n",
      "|   800/  900 batches | ms/batch 12.18 | loss  1.22 | perplexity     3.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.65s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.38 | loss  1.22 | perplexity     3.40\n",
      "|   400/  900 batches | ms/batch 12.34 | loss  1.22 | perplexity     3.40\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  1.21 | perplexity     3.36\n",
      "|   800/  900 batches | ms/batch 12.24 | loss  1.20 | perplexity     3.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.74s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.33 | loss  1.20 | perplexity     3.32\n",
      "|   400/  900 batches | ms/batch 12.17 | loss  1.17 | perplexity     3.21\n",
      "|   600/  900 batches | ms/batch 12.68 | loss  1.14 | perplexity     3.14\n",
      "|   800/  900 batches | ms/batch 12.51 | loss  1.11 | perplexity     3.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.01s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  1.08 | perplexity     2.95\n",
      "|   400/  900 batches | ms/batch 12.31 | loss  1.06 | perplexity     2.88\n",
      "|   600/  900 batches | ms/batch 12.20 | loss  1.05 | perplexity     2.86\n",
      "|   800/  900 batches | ms/batch 12.04 | loss  1.04 | perplexity     2.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 12.86s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.16 | loss  1.03 | perplexity     2.79\n",
      "|   400/  900 batches | ms/batch 13.06 | loss  1.01 | perplexity     2.75\n",
      "|   600/  900 batches | ms/batch 12.62 | loss  1.01 | perplexity     2.76\n",
      "|   800/  900 batches | ms/batch 12.71 | loss  1.00 | perplexity     2.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 13.21s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.73 | loss  1.00 | perplexity     2.72\n",
      "|   400/  900 batches | ms/batch 12.66 | loss  0.99 | perplexity     2.68\n",
      "|   600/  900 batches | ms/batch 12.74 | loss  0.98 | perplexity     2.65\n",
      "|   800/  900 batches | ms/batch 12.94 | loss  0.97 | perplexity     2.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 13.30s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.48 | loss  0.97 | perplexity     2.64\n",
      "|   400/  900 batches | ms/batch 12.08 | loss  0.96 | perplexity     2.60\n",
      "|   600/  900 batches | ms/batch 12.45 | loss  0.96 | perplexity     2.60\n",
      "|   800/  900 batches | ms/batch 12.51 | loss  0.96 | perplexity     2.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 12.79s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.93 | loss  0.96 | perplexity     2.60\n",
      "|   400/  900 batches | ms/batch 11.70 | loss  0.95 | perplexity     2.58\n",
      "|   600/  900 batches | ms/batch 11.60 | loss  0.93 | perplexity     2.53\n",
      "|   800/  900 batches | ms/batch 11.67 | loss  0.93 | perplexity     2.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 12.28s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.65 | loss  0.93 | perplexity     2.52\n",
      "|   400/  900 batches | ms/batch 11.58 | loss  0.92 | perplexity     2.51\n",
      "|   600/  900 batches | ms/batch 11.59 | loss  0.93 | perplexity     2.53\n",
      "|   800/  900 batches | ms/batch 11.60 | loss  0.92 | perplexity     2.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 12.17s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.68 | loss  0.92 | perplexity     2.50\n",
      "|   400/  900 batches | ms/batch 11.60 | loss  0.91 | perplexity     2.48\n",
      "|   600/  900 batches | ms/batch 11.60 | loss  0.91 | perplexity     2.47\n",
      "|   800/  900 batches | ms/batch 12.38 | loss  0.90 | perplexity     2.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.45s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.50 | loss  0.90 | perplexity     2.47\n",
      "|   400/  900 batches | ms/batch 12.45 | loss  0.89 | perplexity     2.44\n",
      "|   600/  900 batches | ms/batch 12.51 | loss  0.90 | perplexity     2.47\n",
      "|   800/  900 batches | ms/batch 12.47 | loss  0.89 | perplexity     2.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 12.96s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.58 | loss  0.92 | perplexity     2.50\n",
      "|   400/  900 batches | ms/batch 12.47 | loss  0.89 | perplexity     2.45\n",
      "|   600/  900 batches | ms/batch 12.66 | loss  0.88 | perplexity     2.42\n",
      "|   800/  900 batches | ms/batch 12.48 | loss  0.88 | perplexity     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 13.09s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.64 | loss  0.89 | perplexity     2.43\n",
      "|   400/  900 batches | ms/batch 12.47 | loss  0.88 | perplexity     2.41\n",
      "|   600/  900 batches | ms/batch 12.46 | loss  0.88 | perplexity     2.41\n",
      "|   800/  900 batches | ms/batch 12.66 | loss  0.87 | perplexity     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 13.12s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.55 | loss  0.89 | perplexity     2.43\n",
      "|   400/  900 batches | ms/batch 12.05 | loss  0.88 | perplexity     2.41\n",
      "|   600/  900 batches | ms/batch 12.67 | loss  0.88 | perplexity     2.40\n",
      "|   800/  900 batches | ms/batch 12.65 | loss  0.87 | perplexity     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 13.06s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.09 | loss  0.87 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 12.01 | loss  0.87 | perplexity     2.38\n",
      "|   600/  900 batches | ms/batch 12.02 | loss  0.88 | perplexity     2.40\n",
      "|   800/  900 batches | ms/batch 12.01 | loss  0.90 | perplexity     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 12.67s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.46 | loss  0.88 | perplexity     2.41\n",
      "|   400/  900 batches | ms/batch 13.07 | loss  0.88 | perplexity     2.40\n",
      "|   600/  900 batches | ms/batch 12.81 | loss  0.88 | perplexity     2.40\n",
      "|   800/  900 batches | ms/batch 13.04 | loss  0.87 | perplexity     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 13.42s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.51 | loss  0.87 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 13.39 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 13.56 | loss  0.87 | perplexity     2.38\n",
      "|   800/  900 batches | ms/batch 13.47 | loss  0.88 | perplexity     2.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 14.12s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.92 | loss  0.87 | perplexity     2.39\n",
      "|   400/  900 batches | ms/batch 13.75 | loss  0.86 | perplexity     2.36\n",
      "|   600/  900 batches | ms/batch 13.47 | loss  0.86 | perplexity     2.37\n",
      "|   800/  900 batches | ms/batch 13.38 | loss  0.87 | perplexity     2.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 14.12s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.38 | loss  0.86 | perplexity     2.37\n",
      "|   400/  900 batches | ms/batch 13.35 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 13.58 | loss  0.86 | perplexity     2.36\n",
      "|   800/  900 batches | ms/batch 13.42 | loss  0.86 | perplexity     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 14.07s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.29 | loss  0.86 | perplexity     2.37\n",
      "|   400/  900 batches | ms/batch 12.98 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 13.27 | loss  0.85 | perplexity     2.35\n",
      "|   800/  900 batches | ms/batch 13.49 | loss  0.86 | perplexity     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 13.81s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.84 | loss  0.86 | perplexity     2.36\n",
      "|   400/  900 batches | ms/batch 13.75 | loss  0.86 | perplexity     2.35\n",
      "|   600/  900 batches | ms/batch 13.44 | loss  0.85 | perplexity     2.35\n",
      "|   800/  900 batches | ms/batch 13.79 | loss  0.86 | perplexity     2.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 14.09s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.49 | loss  0.86 | perplexity     2.36\n",
      "|   400/  900 batches | ms/batch 13.06 | loss  0.86 | perplexity     2.36\n",
      "|   600/  900 batches | ms/batch 13.46 | loss  0.86 | perplexity     2.36\n",
      "|   800/  900 batches | ms/batch 13.28 | loss  0.85 | perplexity     2.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 14.01s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.94 | loss  0.84 | perplexity     2.32\n",
      "|   400/  900 batches | ms/batch 13.21 | loss  0.83 | perplexity     2.30\n",
      "|   600/  900 batches | ms/batch 13.27 | loss  0.82 | perplexity     2.27\n",
      "|   800/  900 batches | ms/batch 13.46 | loss  0.81 | perplexity     2.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 13.95s | test accuracy  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.79 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 13.36 | loss  0.77 | perplexity     2.16\n",
      "|   600/  900 batches | ms/batch 13.61 | loss  0.77 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 13.26 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 14.27s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.94 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 13.31 | loss  0.73 | perplexity     2.08\n",
      "|   600/  900 batches | ms/batch 13.81 | loss  0.73 | perplexity     2.07\n",
      "|   800/  900 batches | ms/batch 13.48 | loss  0.72 | perplexity     2.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 14.16s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.59 | loss  0.70 | perplexity     2.01\n",
      "|   400/  900 batches | ms/batch 13.40 | loss  0.69 | perplexity     2.00\n",
      "|   600/  900 batches | ms/batch 13.44 | loss  0.69 | perplexity     2.00\n",
      "|   800/  900 batches | ms/batch 13.79 | loss  0.69 | perplexity     2.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 14.12s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.52 | loss  0.68 | perplexity     1.98\n",
      "|   400/  900 batches | ms/batch 13.04 | loss  0.68 | perplexity     1.98\n",
      "|   600/  900 batches | ms/batch 13.27 | loss  0.66 | perplexity     1.93\n",
      "|   800/  900 batches | ms/batch 13.19 | loss  0.66 | perplexity     1.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 13.98s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.65 | loss  0.67 | perplexity     1.95\n",
      "|   400/  900 batches | ms/batch 13.49 | loss  0.65 | perplexity     1.92\n",
      "|   600/  900 batches | ms/batch 13.74 | loss  0.65 | perplexity     1.91\n",
      "|   800/  900 batches | ms/batch 13.52 | loss  0.65 | perplexity     1.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 13.97s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 14.02 | loss  0.65 | perplexity     1.92\n",
      "|   400/  900 batches | ms/batch 13.45 | loss  0.64 | perplexity     1.89\n",
      "|   600/  900 batches | ms/batch 13.11 | loss  0.64 | perplexity     1.89\n",
      "|   800/  900 batches | ms/batch 13.65 | loss  0.63 | perplexity     1.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 14.13s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.65 | loss  0.64 | perplexity     1.89\n",
      "|   400/  900 batches | ms/batch 13.51 | loss  0.63 | perplexity     1.87\n",
      "|   600/  900 batches | ms/batch 13.20 | loss  0.62 | perplexity     1.86\n",
      "|   800/  900 batches | ms/batch 13.54 | loss  0.63 | perplexity     1.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 14.13s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.42 | loss  0.63 | perplexity     1.87\n",
      "|   400/  900 batches | ms/batch 13.29 | loss  0.61 | perplexity     1.85\n",
      "|   600/  900 batches | ms/batch 13.63 | loss  0.62 | perplexity     1.85\n",
      "|   800/  900 batches | ms/batch 13.13 | loss  0.62 | perplexity     1.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 14.00s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.69 | loss  0.61 | perplexity     1.84\n",
      "|   400/  900 batches | ms/batch 13.18 | loss  0.60 | perplexity     1.82\n",
      "|   600/  900 batches | ms/batch 13.43 | loss  0.61 | perplexity     1.85\n",
      "|   800/  900 batches | ms/batch 13.32 | loss  0.62 | perplexity     1.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 13.99s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.79 | loss  0.61 | perplexity     1.84\n",
      "|   400/  900 batches | ms/batch 13.88 | loss  0.61 | perplexity     1.84\n",
      "|   600/  900 batches | ms/batch 13.54 | loss  0.60 | perplexity     1.82\n",
      "|   800/  900 batches | ms/batch 13.33 | loss  0.60 | perplexity     1.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 14.21s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.83 | loss  0.60 | perplexity     1.82\n",
      "|   400/  900 batches | ms/batch 13.77 | loss  0.60 | perplexity     1.81\n",
      "|   600/  900 batches | ms/batch 13.48 | loss  0.59 | perplexity     1.81\n",
      "|   800/  900 batches | ms/batch 13.74 | loss  0.58 | perplexity     1.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 14.27s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.44 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 13.35 | loss  0.59 | perplexity     1.81\n",
      "|   600/  900 batches | ms/batch 13.53 | loss  0.59 | perplexity     1.80\n",
      "|   800/  900 batches | ms/batch 13.47 | loss  0.59 | perplexity     1.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 14.10s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 14.17 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 13.50 | loss  0.58 | perplexity     1.78\n",
      "|   600/  900 batches | ms/batch 13.48 | loss  0.58 | perplexity     1.79\n",
      "|   800/  900 batches | ms/batch 13.47 | loss  0.59 | perplexity     1.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 14.23s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.93 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 13.31 | loss  0.58 | perplexity     1.79\n",
      "|   600/  900 batches | ms/batch 13.52 | loss  0.57 | perplexity     1.77\n",
      "|   800/  900 batches | ms/batch 13.46 | loss  0.58 | perplexity     1.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 14.16s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.66 | loss  0.58 | perplexity     1.79\n",
      "|   400/  900 batches | ms/batch 14.29 | loss  0.57 | perplexity     1.77\n",
      "|   600/  900 batches | ms/batch 13.55 | loss  0.57 | perplexity     1.77\n",
      "|   800/  900 batches | ms/batch 13.86 | loss  0.56 | perplexity     1.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 14.26s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.49 | loss  0.57 | perplexity     1.78\n",
      "|   400/  900 batches | ms/batch 13.46 | loss  0.56 | perplexity     1.76\n",
      "|   600/  900 batches | ms/batch 14.09 | loss  0.57 | perplexity     1.78\n",
      "|   800/  900 batches | ms/batch 13.83 | loss  0.56 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 14.26s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.76 | loss  0.57 | perplexity     1.78\n",
      "|   400/  900 batches | ms/batch 13.89 | loss  0.56 | perplexity     1.75\n",
      "|   600/  900 batches | ms/batch 13.58 | loss  0.57 | perplexity     1.77\n",
      "|   800/  900 batches | ms/batch 13.78 | loss  0.56 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 14.34s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.83 | loss  0.57 | perplexity     1.77\n",
      "|   400/  900 batches | ms/batch 13.62 | loss  0.56 | perplexity     1.75\n",
      "|   600/  900 batches | ms/batch 14.07 | loss  0.56 | perplexity     1.75\n",
      "|   800/  900 batches | ms/batch 13.51 | loss  0.55 | perplexity     1.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 14.25s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.74 | loss  0.56 | perplexity     1.75\n",
      "|   400/  900 batches | ms/batch 13.32 | loss  0.56 | perplexity     1.75\n",
      "|   600/  900 batches | ms/batch 13.39 | loss  0.55 | perplexity     1.73\n",
      "|   800/  900 batches | ms/batch 13.23 | loss  0.56 | perplexity     1.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 14.00s | test accuracy  0.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.50 | loss  0.55 | perplexity     1.73\n",
      "|   400/  900 batches | ms/batch 13.15 | loss  0.54 | perplexity     1.72\n",
      "|   600/  900 batches | ms/batch 13.54 | loss  0.55 | perplexity     1.73\n",
      "|   800/  900 batches | ms/batch 13.23 | loss  0.54 | perplexity     1.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 14.02s | test accuracy  0.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.85 | loss  0.54 | perplexity     1.71\n",
      "|   400/  900 batches | ms/batch 13.64 | loss  0.54 | perplexity     1.72\n",
      "|   600/  900 batches | ms/batch 13.63 | loss  0.53 | perplexity     1.71\n",
      "|   800/  900 batches | ms/batch 13.26 | loss  0.54 | perplexity     1.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 14.16s | test accuracy  0.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.47 | loss  0.53 | perplexity     1.70\n",
      "|   400/  900 batches | ms/batch 13.33 | loss  0.52 | perplexity     1.69\n",
      "|   600/  900 batches | ms/batch 13.28 | loss  0.52 | perplexity     1.69\n",
      "|   800/  900 batches | ms/batch 13.34 | loss  0.52 | perplexity     1.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 14.05s | test accuracy  0.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.81 | loss  0.53 | perplexity     1.69\n",
      "|   400/  900 batches | ms/batch 13.45 | loss  0.50 | perplexity     1.65\n",
      "|   600/  900 batches | ms/batch 13.55 | loss  0.49 | perplexity     1.63\n",
      "|   800/  900 batches | ms/batch 13.47 | loss  0.49 | perplexity     1.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 14.16s | test accuracy  0.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.77 | loss  0.47 | perplexity     1.60\n",
      "|   400/  900 batches | ms/batch 13.36 | loss  0.47 | perplexity     1.60\n",
      "|   600/  900 batches | ms/batch 13.64 | loss  0.46 | perplexity     1.58\n",
      "|   800/  900 batches | ms/batch 13.45 | loss  0.45 | perplexity     1.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 14.05s | test accuracy  0.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.37 | loss  0.42 | perplexity     1.51\n",
      "|   400/  900 batches | ms/batch 13.11 | loss  0.39 | perplexity     1.48\n",
      "|   600/  900 batches | ms/batch 13.64 | loss  0.37 | perplexity     1.45\n",
      "|   800/  900 batches | ms/batch 13.56 | loss  0.37 | perplexity     1.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 14.04s | test accuracy  0.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 14.03 | loss  0.34 | perplexity     1.40\n",
      "|   400/  900 batches | ms/batch 13.61 | loss  0.33 | perplexity     1.39\n",
      "|   600/  900 batches | ms/batch 13.74 | loss  0.32 | perplexity     1.38\n",
      "|   800/  900 batches | ms/batch 13.73 | loss  0.31 | perplexity     1.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 14.25s | test accuracy  0.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.19 | loss  0.30 | perplexity     1.36\n",
      "|   400/  900 batches | ms/batch 13.56 | loss  0.30 | perplexity     1.35\n",
      "|   600/  900 batches | ms/batch 13.58 | loss  0.30 | perplexity     1.35\n",
      "|   800/  900 batches | ms/batch 13.68 | loss  0.29 | perplexity     1.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 14.02s | test accuracy  0.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.80 | loss  0.28 | perplexity     1.32\n",
      "|   400/  900 batches | ms/batch 13.63 | loss  0.26 | perplexity     1.30\n",
      "|   600/  900 batches | ms/batch 13.64 | loss  0.25 | perplexity     1.28\n",
      "|   800/  900 batches | ms/batch 13.65 | loss  0.24 | perplexity     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 14.17s | test accuracy  0.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.88 | loss  0.25 | perplexity     1.29\n",
      "|   400/  900 batches | ms/batch 13.44 | loss  0.23 | perplexity     1.26\n",
      "|   600/  900 batches | ms/batch 13.28 | loss  0.28 | perplexity     1.32\n",
      "|   800/  900 batches | ms/batch 13.67 | loss  0.25 | perplexity     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 14.13s | test accuracy  0.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.56 | loss  0.25 | perplexity     1.29\n",
      "|   400/  900 batches | ms/batch 13.36 | loss  0.23 | perplexity     1.26\n",
      "|   600/  900 batches | ms/batch 13.61 | loss  0.24 | perplexity     1.27\n",
      "|   800/  900 batches | ms/batch 13.30 | loss  0.22 | perplexity     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 13.99s | test accuracy  0.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.77 | loss  0.22 | perplexity     1.25\n",
      "|   400/  900 batches | ms/batch 13.78 | loss  0.22 | perplexity     1.24\n",
      "|   600/  900 batches | ms/batch 13.53 | loss  0.22 | perplexity     1.25\n",
      "|   800/  900 batches | ms/batch 13.73 | loss  0.21 | perplexity     1.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 14.19s | test accuracy  0.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.41 | loss  0.21 | perplexity     1.24\n",
      "|   400/  900 batches | ms/batch 13.86 | loss  0.21 | perplexity     1.23\n",
      "|   600/  900 batches | ms/batch 13.52 | loss  0.20 | perplexity     1.22\n",
      "|   800/  900 batches | ms/batch 13.15 | loss  0.20 | perplexity     1.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 13.96s | test accuracy  0.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.27 | loss  0.20 | perplexity     1.22\n",
      "|   400/  900 batches | ms/batch 13.27 | loss  0.18 | perplexity     1.20\n",
      "|   600/  900 batches | ms/batch 13.85 | loss  0.18 | perplexity     1.20\n",
      "|   800/  900 batches | ms/batch 13.52 | loss  0.22 | perplexity     1.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 13.90s | test accuracy  0.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.72 | loss  0.20 | perplexity     1.22\n",
      "|   400/  900 batches | ms/batch 13.76 | loss  0.19 | perplexity     1.21\n",
      "|   600/  900 batches | ms/batch 13.43 | loss  0.18 | perplexity     1.20\n",
      "|   800/  900 batches | ms/batch 13.74 | loss  0.18 | perplexity     1.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 14.07s | test accuracy  0.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.40 | loss  0.18 | perplexity     1.19\n",
      "|   400/  900 batches | ms/batch 13.39 | loss  0.18 | perplexity     1.19\n",
      "|   600/  900 batches | ms/batch 13.62 | loss  0.18 | perplexity     1.19\n",
      "|   800/  900 batches | ms/batch 13.27 | loss  0.20 | perplexity     1.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 14.08s | test accuracy  0.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.74 | loss  0.18 | perplexity     1.20\n",
      "|   400/  900 batches | ms/batch 13.46 | loss  0.18 | perplexity     1.19\n",
      "|   600/  900 batches | ms/batch 13.46 | loss  0.19 | perplexity     1.20\n",
      "|   800/  900 batches | ms/batch 13.81 | loss  0.18 | perplexity     1.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 14.12s | test accuracy  0.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.46 | loss  0.18 | perplexity     1.20\n",
      "|   400/  900 batches | ms/batch 13.78 | loss  0.16 | perplexity     1.18\n",
      "|   600/  900 batches | ms/batch 13.40 | loss  0.16 | perplexity     1.17\n",
      "|   800/  900 batches | ms/batch 13.35 | loss  0.20 | perplexity     1.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 14.07s | test accuracy  0.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.47 | loss  0.17 | perplexity     1.18\n",
      "|   400/  900 batches | ms/batch 13.65 | loss  0.16 | perplexity     1.17\n",
      "|   600/  900 batches | ms/batch 13.46 | loss  0.16 | perplexity     1.17\n",
      "|   800/  900 batches | ms/batch 13.72 | loss  0.16 | perplexity     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 14.15s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.50 | loss  0.16 | perplexity     1.17\n",
      "|   400/  900 batches | ms/batch 13.59 | loss  0.16 | perplexity     1.17\n",
      "|   600/  900 batches | ms/batch 13.33 | loss  0.15 | perplexity     1.16\n",
      "|   800/  900 batches | ms/batch 13.58 | loss  0.16 | perplexity     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 14.11s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 14.04 | loss  0.15 | perplexity     1.16\n",
      "|   400/  900 batches | ms/batch 13.86 | loss  0.14 | perplexity     1.16\n",
      "|   600/  900 batches | ms/batch 13.78 | loss  0.15 | perplexity     1.17\n",
      "|   800/  900 batches | ms/batch 14.04 | loss  0.15 | perplexity     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 14.50s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.62 | loss  0.15 | perplexity     1.17\n",
      "|   400/  900 batches | ms/batch 13.57 | loss  0.16 | perplexity     1.17\n",
      "|   600/  900 batches | ms/batch 13.12 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 13.57 | loss  0.15 | perplexity     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 14.09s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.93 | loss  0.15 | perplexity     1.16\n",
      "|   400/  900 batches | ms/batch 13.35 | loss  0.14 | perplexity     1.16\n",
      "|   600/  900 batches | ms/batch 13.40 | loss  0.15 | perplexity     1.16\n",
      "|   800/  900 batches | ms/batch 13.45 | loss  0.15 | perplexity     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 13.98s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.40 | loss  0.14 | perplexity     1.15\n",
      "|   400/  900 batches | ms/batch 13.53 | loss  0.14 | perplexity     1.15\n",
      "|   600/  900 batches | ms/batch 13.60 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 13.64 | loss  0.14 | perplexity     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 13.98s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.48 | loss  0.14 | perplexity     1.15\n",
      "|   400/  900 batches | ms/batch 13.62 | loss  0.13 | perplexity     1.14\n",
      "|   600/  900 batches | ms/batch 13.34 | loss  0.15 | perplexity     1.16\n",
      "|   800/  900 batches | ms/batch 13.79 | loss  0.14 | perplexity     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 14.06s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.47 | loss  0.14 | perplexity     1.15\n",
      "|   400/  900 batches | ms/batch 13.57 | loss  0.15 | perplexity     1.16\n",
      "|   600/  900 batches | ms/batch 13.62 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 13.49 | loss  0.13 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 14.18s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.51 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 14.00 | loss  0.13 | perplexity     1.14\n",
      "|   600/  900 batches | ms/batch 13.39 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 13.16 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 14.10s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.35 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 13.65 | loss  0.13 | perplexity     1.14\n",
      "|   600/  900 batches | ms/batch 13.73 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 13.51 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 13.93s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.59 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 13.27 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 13.45 | loss  0.12 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 13.56 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 13.98s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.36 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 13.52 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 13.73 | loss  0.11 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 13.63 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 14.05s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.60 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 13.76 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 13.92 | loss  0.12 | perplexity     1.13\n",
      "|   800/  900 batches | ms/batch 13.34 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 14.20s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.55 | loss  0.13 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 13.53 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 13.79 | loss  0.11 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 13.63 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 14.06s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.19 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 13.37 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 13.60 | loss  0.12 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 13.43 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 13.87s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.64 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 13.63 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 13.40 | loss  0.12 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 13.65 | loss  0.11 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 14.14s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.44 | loss  0.11 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 13.72 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 13.19 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 12.14 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 13.39s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.30 | loss  0.12 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 12.09 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 12.22 | loss  0.12 | perplexity     1.13\n",
      "|   800/  900 batches | ms/batch 12.20 | loss  0.12 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 12.68s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.63 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 11.37 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 11.36 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 11.42 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 11.98s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.45 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 11.37 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 11.41 | loss  0.12 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 11.37 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 11.99s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.43 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.10 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.55 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 12.63 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 12.89s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.69 | loss  0.12 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 11.60 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 11.60 | loss  0.11 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 11.62 | loss  0.11 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 12.19s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.68 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 12.00 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.64 | loss  0.10 | perplexity     1.10\n",
      "|   800/  900 batches | ms/batch 12.28 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 12.67s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  0.11 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 12.39 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 12.00 | loss  0.11 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 12.84 | loss  0.10 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 13.04s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.49 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.75 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 11.98 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 12.72 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 13.08s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.93 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.72 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 12.86 | loss  0.10 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 12.64 | loss  0.10 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 13.24s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.49 | loss  0.11 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 12.62 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 13.29 | loss  0.11 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 12.78 | loss  0.11 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 13.22s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.39 | loss  0.10 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  0.10 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  0.11 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 12.69s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.60 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.11 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.52 | loss  0.12 | perplexity     1.13\n",
      "|   800/  900 batches | ms/batch 12.49 | loss  0.10 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 12.78s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.38 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.12 | loss  0.10 | perplexity     1.10\n",
      "|   600/  900 batches | ms/batch 12.30 | loss  0.10 | perplexity     1.10\n",
      "|   800/  900 batches | ms/batch 12.24 | loss  0.11 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 12.75s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.37 | loss  0.11 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 12.68 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.29 | loss  0.11 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 12.52 | loss  0.09 | perplexity     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 13.04s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.44 | loss  0.10 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.70 | loss  0.10 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.72 | loss  0.12 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 12.84 | loss  0.12 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 13.22s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.28 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.13 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.10 | loss  0.10 | perplexity     1.10\n",
      "|   800/  900 batches | ms/batch 12.91 | loss  0.09 | perplexity     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 12.91s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "acc_v_epoch = train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d9d440",
    "outputId": "1872232b-b120-440b-e1a6-666e079efa3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3154+492=8076\t actual result: 3646\n",
      "6256+8737=14993\t actual result: 14993\n",
      "7713+1406=9119\t actual result: 9119\n",
      "7999+10=9008\t actual result: 8009\n",
      "4599+9307=13906\t actual result: 13906\n",
      "757+3294=1151\t actual result: 4051\n",
      "3307+8023=11330\t actual result: 11330\n",
      "8438+1533=9971\t actual result: 9971\n",
      "5668+2084=7752\t actual result: 7752\n",
      "8801+882=1762\t actual result: 9683\n",
      "2473+6077=8550\t actual result: 8550\n",
      "2407+6007=8414\t actual result: 8414\n",
      "7129+3925=11054\t actual result: 11054\n",
      "1779+2762=4541\t actual result: 4541\n",
      "2031+3954=5985\t actual result: 5985\n",
      "4765+417=8942\t actual result: 5182\n",
      "3027+2130=5157\t actual result: 5157\n",
      "8124+4912=13036\t actual result: 13036\n",
      "6399+3587=9986\t actual result: 9986\n",
      "8874+2565=11439\t actual result: 11439\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(20):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers)).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ec1eafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(acc_v_epoch, label={number_bits})\n",
    "ax.set_title(f\"test accuracy vs epoch\")\n",
    "ax.set_xlabel(f\"epochs\")\n",
    "ax.set_ylabel(f\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25594b26",
   "metadata": {},
   "source": [
    "# Step 5 : testing different number of bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5590bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training 50 epochs on 3 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin.deporte/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.77 | loss  1.76 | perplexity     5.84\n",
      "|   400/  900 batches | ms/batch 11.28 | loss  1.43 | perplexity     4.17\n",
      "|   600/  900 batches | ms/batch 11.27 | loss  1.30 | perplexity     3.68\n",
      "|   800/  900 batches | ms/batch 11.27 | loss  1.23 | perplexity     3.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 11.68s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.34 | loss  1.18 | perplexity     3.26\n",
      "|   400/  900 batches | ms/batch 11.26 | loss  1.14 | perplexity     3.13\n",
      "|   600/  900 batches | ms/batch 11.27 | loss  1.12 | perplexity     3.06\n",
      "|   800/  900 batches | ms/batch 11.26 | loss  1.11 | perplexity     3.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 11.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.34 | loss  1.11 | perplexity     3.05\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  1.09 | perplexity     2.96\n",
      "|   600/  900 batches | ms/batch 11.28 | loss  1.08 | perplexity     2.93\n",
      "|   800/  900 batches | ms/batch 11.27 | loss  1.08 | perplexity     2.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 11.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.33 | loss  1.08 | perplexity     2.93\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  1.06 | perplexity     2.89\n",
      "|   600/  900 batches | ms/batch 11.26 | loss  1.07 | perplexity     2.92\n",
      "|   800/  900 batches | ms/batch 11.28 | loss  1.06 | perplexity     2.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 11.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.33 | loss  1.05 | perplexity     2.86\n",
      "|   400/  900 batches | ms/batch 11.26 | loss  1.04 | perplexity     2.84\n",
      "|   600/  900 batches | ms/batch 11.26 | loss  1.05 | perplexity     2.86\n",
      "|   800/  900 batches | ms/batch 11.26 | loss  1.05 | perplexity     2.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 11.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.33 | loss  1.05 | perplexity     2.84\n",
      "|   400/  900 batches | ms/batch 11.26 | loss  1.04 | perplexity     2.84\n",
      "|   600/  900 batches | ms/batch 11.27 | loss  1.02 | perplexity     2.77\n",
      "|   800/  900 batches | ms/batch 11.27 | loss  1.02 | perplexity     2.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 11.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.34 | loss  1.03 | perplexity     2.80\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  1.02 | perplexity     2.78\n",
      "|   600/  900 batches | ms/batch 11.78 | loss  1.01 | perplexity     2.76\n",
      "|   800/  900 batches | ms/batch 11.29 | loss  1.01 | perplexity     2.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 11.68s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.34 | loss  1.02 | perplexity     2.77\n",
      "|   400/  900 batches | ms/batch 11.33 | loss  0.99 | perplexity     2.70\n",
      "|   600/  900 batches | ms/batch 11.28 | loss  1.01 | perplexity     2.76\n",
      "|   800/  900 batches | ms/batch 11.90 | loss  0.99 | perplexity     2.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 11.78s | test accuracy  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.35 | loss  1.00 | perplexity     2.72\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  0.98 | perplexity     2.67\n",
      "|   600/  900 batches | ms/batch 11.37 | loss  0.98 | perplexity     2.67\n",
      "|   800/  900 batches | ms/batch 11.30 | loss  0.95 | perplexity     2.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 11.61s | test accuracy  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.37 | loss  0.90 | perplexity     2.47\n",
      "|   400/  900 batches | ms/batch 11.30 | loss  0.88 | perplexity     2.42\n",
      "|   600/  900 batches | ms/batch 11.30 | loss  0.84 | perplexity     2.31\n",
      "|   800/  900 batches | ms/batch 11.82 | loss  0.81 | perplexity     2.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 11.79s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.07 | loss  0.80 | perplexity     2.23\n",
      "|   400/  900 batches | ms/batch 11.52 | loss  0.77 | perplexity     2.16\n",
      "|   600/  900 batches | ms/batch 11.28 | loss  0.79 | perplexity     2.20\n",
      "|   800/  900 batches | ms/batch 11.28 | loss  0.78 | perplexity     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 11.78s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.35 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 11.49 | loss  0.73 | perplexity     2.08\n",
      "|   600/  900 batches | ms/batch 11.94 | loss  0.71 | perplexity     2.04\n",
      "|   800/  900 batches | ms/batch 11.86 | loss  0.73 | perplexity     2.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 11.97s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.20 | loss  0.72 | perplexity     2.05\n",
      "|   400/  900 batches | ms/batch 12.18 | loss  0.71 | perplexity     2.03\n",
      "|   600/  900 batches | ms/batch 12.17 | loss  0.69 | perplexity     2.00\n",
      "|   800/  900 batches | ms/batch 12.09 | loss  0.68 | perplexity     1.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.35s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.09 | loss  0.68 | perplexity     1.97\n",
      "|   400/  900 batches | ms/batch 11.92 | loss  0.67 | perplexity     1.95\n",
      "|   600/  900 batches | ms/batch 11.92 | loss  0.66 | perplexity     1.94\n",
      "|   800/  900 batches | ms/batch 11.82 | loss  0.67 | perplexity     1.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 12.17s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.97 | loss  0.66 | perplexity     1.94\n",
      "|   400/  900 batches | ms/batch 11.87 | loss  0.63 | perplexity     1.88\n",
      "|   600/  900 batches | ms/batch 12.08 | loss  0.63 | perplexity     1.87\n",
      "|   800/  900 batches | ms/batch 11.84 | loss  0.66 | perplexity     1.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 12.17s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.05 | loss  0.63 | perplexity     1.88\n",
      "|   400/  900 batches | ms/batch 11.88 | loss  0.63 | perplexity     1.89\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  0.60 | perplexity     1.83\n",
      "|   800/  900 batches | ms/batch 11.87 | loss  0.59 | perplexity     1.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 12.19s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.96 | loss  0.61 | perplexity     1.84\n",
      "|   400/  900 batches | ms/batch 11.89 | loss  0.60 | perplexity     1.82\n",
      "|   600/  900 batches | ms/batch 11.93 | loss  0.61 | perplexity     1.83\n",
      "|   800/  900 batches | ms/batch 11.90 | loss  0.60 | perplexity     1.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.16s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.04 | loss  0.60 | perplexity     1.83\n",
      "|   400/  900 batches | ms/batch 11.91 | loss  0.58 | perplexity     1.79\n",
      "|   600/  900 batches | ms/batch 11.93 | loss  0.58 | perplexity     1.79\n",
      "|   800/  900 batches | ms/batch 11.28 | loss  0.59 | perplexity     1.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 11.98s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.61 | loss  0.58 | perplexity     1.79\n",
      "|   400/  900 batches | ms/batch 11.90 | loss  0.58 | perplexity     1.79\n",
      "|   600/  900 batches | ms/batch 12.03 | loss  0.57 | perplexity     1.77\n",
      "|   800/  900 batches | ms/batch 11.96 | loss  0.56 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.14s | test accuracy  0.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.07 | loss  0.56 | perplexity     1.76\n",
      "|   400/  900 batches | ms/batch 11.94 | loss  0.54 | perplexity     1.72\n",
      "|   600/  900 batches | ms/batch 11.95 | loss  0.54 | perplexity     1.71\n",
      "|   800/  900 batches | ms/batch 11.93 | loss  0.56 | perplexity     1.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.23s | test accuracy  0.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.03 | loss  0.55 | perplexity     1.73\n",
      "|   400/  900 batches | ms/batch 12.43 | loss  0.54 | perplexity     1.71\n",
      "|   600/  900 batches | ms/batch 12.43 | loss  0.50 | perplexity     1.64\n",
      "|   800/  900 batches | ms/batch 12.41 | loss  0.47 | perplexity     1.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 12.61s | test accuracy  0.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.30 | loss  0.44 | perplexity     1.55\n",
      "|   400/  900 batches | ms/batch 11.92 | loss  0.41 | perplexity     1.51\n",
      "|   600/  900 batches | ms/batch 12.08 | loss  0.41 | perplexity     1.50\n",
      "|   800/  900 batches | ms/batch 12.00 | loss  0.39 | perplexity     1.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 12.26s | test accuracy  0.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.53 | loss  0.37 | perplexity     1.45\n",
      "|   400/  900 batches | ms/batch 11.67 | loss  0.35 | perplexity     1.42\n",
      "|   600/  900 batches | ms/batch 12.24 | loss  0.34 | perplexity     1.41\n",
      "|   800/  900 batches | ms/batch 12.27 | loss  0.32 | perplexity     1.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 12.26s | test accuracy  0.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.81 | loss  0.27 | perplexity     1.32\n",
      "|   400/  900 batches | ms/batch 11.71 | loss  0.25 | perplexity     1.28\n",
      "|   600/  900 batches | ms/batch 11.71 | loss  0.24 | perplexity     1.28\n",
      "|   800/  900 batches | ms/batch 11.89 | loss  0.23 | perplexity     1.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 12.10s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.77 | loss  0.22 | perplexity     1.24\n",
      "|   400/  900 batches | ms/batch 11.72 | loss  0.21 | perplexity     1.23\n",
      "|   600/  900 batches | ms/batch 11.72 | loss  0.20 | perplexity     1.22\n",
      "|   800/  900 batches | ms/batch 11.73 | loss  0.18 | perplexity     1.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 12.05s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.79 | loss  0.19 | perplexity     1.21\n",
      "|   400/  900 batches | ms/batch 11.71 | loss  0.17 | perplexity     1.19\n",
      "|   600/  900 batches | ms/batch 11.72 | loss  0.18 | perplexity     1.20\n",
      "|   800/  900 batches | ms/batch 11.72 | loss  0.17 | perplexity     1.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 12.06s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.80 | loss  0.17 | perplexity     1.19\n",
      "|   400/  900 batches | ms/batch 11.73 | loss  0.17 | perplexity     1.19\n",
      "|   600/  900 batches | ms/batch 11.76 | loss  0.16 | perplexity     1.17\n",
      "|   800/  900 batches | ms/batch 11.66 | loss  0.16 | perplexity     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 11.98s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.53 | loss  0.15 | perplexity     1.17\n",
      "|   400/  900 batches | ms/batch 11.45 | loss  0.15 | perplexity     1.17\n",
      "|   600/  900 batches | ms/batch 11.45 | loss  0.17 | perplexity     1.18\n",
      "|   800/  900 batches | ms/batch 11.45 | loss  0.15 | perplexity     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 11.78s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.52 | loss  0.16 | perplexity     1.17\n",
      "|   400/  900 batches | ms/batch 11.51 | loss  0.14 | perplexity     1.15\n",
      "|   600/  900 batches | ms/batch 11.49 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 11.39 | loss  0.16 | perplexity     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 11.76s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.50 | loss  0.16 | perplexity     1.18\n",
      "|   400/  900 batches | ms/batch 11.39 | loss  0.14 | perplexity     1.15\n",
      "|   600/  900 batches | ms/batch 11.39 | loss  0.16 | perplexity     1.17\n",
      "|   800/  900 batches | ms/batch 11.35 | loss  0.15 | perplexity     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 11.70s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.41 | loss  0.14 | perplexity     1.16\n",
      "|   400/  900 batches | ms/batch 11.34 | loss  0.15 | perplexity     1.16\n",
      "|   600/  900 batches | ms/batch 11.35 | loss  0.14 | perplexity     1.15\n",
      "|   800/  900 batches | ms/batch 11.34 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 11.66s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.40 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 11.35 | loss  0.14 | perplexity     1.15\n",
      "|   600/  900 batches | ms/batch 11.37 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 11.35 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 11.66s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.44 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 11.37 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 11.36 | loss  0.12 | perplexity     1.13\n",
      "|   800/  900 batches | ms/batch 11.41 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 11.69s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.44 | loss  0.13 | perplexity     1.14\n",
      "|   400/  900 batches | ms/batch 11.36 | loss  0.12 | perplexity     1.13\n",
      "|   600/  900 batches | ms/batch 11.36 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 11.46 | loss  0.16 | perplexity     1.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 11.86s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.13 | loss  0.14 | perplexity     1.15\n",
      "|   400/  900 batches | ms/batch 11.37 | loss  0.12 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 11.64 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 11.97 | loss  0.15 | perplexity     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 12.11s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.47 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 14.19 | loss  0.12 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 12.98 | loss  0.12 | perplexity     1.13\n",
      "|   800/  900 batches | ms/batch 12.41 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 13.00s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.73 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  0.10 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  0.11 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 12.44 | loss  0.12 | perplexity     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 12.17s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.46 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 11.28 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 11.27 | loss  0.12 | perplexity     1.12\n",
      "|   800/  900 batches | ms/batch 11.71 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 11.89s | test accuracy  0.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.41 | loss  0.11 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 11.26 | loss  0.09 | perplexity     1.10\n",
      "|   600/  900 batches | ms/batch 11.26 | loss  0.11 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 11.26 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 11.59s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.92 | loss  0.11 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 11.95 | loss  0.10 | perplexity     1.10\n",
      "|   600/  900 batches | ms/batch 11.82 | loss  0.10 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 11.81 | loss  0.10 | perplexity     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 12.11s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.80 | loss  0.12 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 11.49 | loss  0.13 | perplexity     1.14\n",
      "|   800/  900 batches | ms/batch 11.27 | loss  0.13 | perplexity     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 11.72s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.33 | loss  0.12 | perplexity     1.13\n",
      "|   400/  900 batches | ms/batch 11.27 | loss  0.11 | perplexity     1.11\n",
      "|   600/  900 batches | ms/batch 11.27 | loss  0.11 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 11.26 | loss  0.15 | perplexity     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 11.57s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.34 | loss  0.11 | perplexity     1.12\n",
      "|   400/  900 batches | ms/batch 11.28 | loss  0.11 | perplexity     1.12\n",
      "|   600/  900 batches | ms/batch 11.58 | loss  0.13 | perplexity     1.13\n",
      "|   800/  900 batches | ms/batch 11.88 | loss  0.11 | perplexity     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 11.91s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.13 | loss  0.10 | perplexity     1.11\n",
      "|   400/  900 batches | ms/batch 12.05 | loss  0.09 | perplexity     1.09\n",
      "|   600/  900 batches | ms/batch 12.00 | loss  0.10 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 11.49 | loss  0.10 | perplexity     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 12.14s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.56 | loss  0.10 | perplexity     1.10\n",
      "|   400/  900 batches | ms/batch 11.49 | loss  0.10 | perplexity     1.10\n",
      "|   600/  900 batches | ms/batch 11.50 | loss  0.09 | perplexity     1.10\n",
      "|   800/  900 batches | ms/batch 11.49 | loss  0.09 | perplexity     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 11.82s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.56 | loss  0.09 | perplexity     1.10\n",
      "|   400/  900 batches | ms/batch 11.50 | loss  0.10 | perplexity     1.10\n",
      "|   600/  900 batches | ms/batch 11.50 | loss  0.10 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 11.51 | loss  0.09 | perplexity     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 11.82s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.56 | loss  0.09 | perplexity     1.09\n",
      "|   400/  900 batches | ms/batch 11.50 | loss  0.09 | perplexity     1.09\n",
      "|   600/  900 batches | ms/batch 11.72 | loss  0.10 | perplexity     1.10\n",
      "|   800/  900 batches | ms/batch 11.93 | loss  0.09 | perplexity     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 11.97s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.11 | loss  0.09 | perplexity     1.10\n",
      "|   400/  900 batches | ms/batch 12.01 | loss  0.09 | perplexity     1.09\n",
      "|   600/  900 batches | ms/batch 11.86 | loss  0.09 | perplexity     1.10\n",
      "|   800/  900 batches | ms/batch 11.51 | loss  0.08 | perplexity     1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 12.24s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.52 | loss  0.09 | perplexity     1.09\n",
      "|   400/  900 batches | ms/batch 12.33 | loss  0.09 | perplexity     1.09\n",
      "|   600/  900 batches | ms/batch 12.45 | loss  0.09 | perplexity     1.09\n",
      "|   800/  900 batches | ms/batch 12.44 | loss  0.14 | perplexity     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 12.70s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.87 | loss  0.10 | perplexity     1.10\n",
      "|   400/  900 batches | ms/batch 12.94 | loss  0.10 | perplexity     1.10\n",
      "|   600/  900 batches | ms/batch 12.75 | loss  0.10 | perplexity     1.11\n",
      "|   800/  900 batches | ms/batch 12.68 | loss  0.10 | perplexity     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 12.97s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "Training 50 epochs on 4 bits\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.29 | loss  1.87 | perplexity     6.50\n",
      "|   400/  900 batches | ms/batch 12.23 | loss  1.59 | perplexity     4.90\n",
      "|   600/  900 batches | ms/batch 12.12 | loss  1.48 | perplexity     4.38\n",
      "|   800/  900 batches | ms/batch 12.24 | loss  1.41 | perplexity     4.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.74s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.85 | loss  1.37 | perplexity     3.95\n",
      "|   400/  900 batches | ms/batch 11.67 | loss  1.34 | perplexity     3.80\n",
      "|   600/  900 batches | ms/batch 11.71 | loss  1.32 | perplexity     3.75\n",
      "|   800/  900 batches | ms/batch 11.61 | loss  1.32 | perplexity     3.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.26s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.68 | loss  1.31 | perplexity     3.71\n",
      "|   400/  900 batches | ms/batch 11.61 | loss  1.29 | perplexity     3.65\n",
      "|   600/  900 batches | ms/batch 11.61 | loss  1.29 | perplexity     3.62\n",
      "|   800/  900 batches | ms/batch 11.61 | loss  1.28 | perplexity     3.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.19s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.68 | loss  1.29 | perplexity     3.63\n",
      "|   400/  900 batches | ms/batch 12.65 | loss  1.28 | perplexity     3.59\n",
      "|   600/  900 batches | ms/batch 12.14 | loss  1.26 | perplexity     3.54\n",
      "|   800/  900 batches | ms/batch 12.94 | loss  1.26 | perplexity     3.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.24s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.90 | loss  1.27 | perplexity     3.56\n",
      "|   400/  900 batches | ms/batch 12.29 | loss  1.25 | perplexity     3.48\n",
      "|   600/  900 batches | ms/batch 12.18 | loss  1.25 | perplexity     3.50\n",
      "|   800/  900 batches | ms/batch 12.01 | loss  1.25 | perplexity     3.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.82s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.60 | loss  1.25 | perplexity     3.49\n",
      "|   400/  900 batches | ms/batch 12.45 | loss  1.24 | perplexity     3.46\n",
      "|   600/  900 batches | ms/batch 12.67 | loss  1.23 | perplexity     3.41\n",
      "|   800/  900 batches | ms/batch 12.42 | loss  1.23 | perplexity     3.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 13.13s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.51 | loss  1.24 | perplexity     3.45\n",
      "|   400/  900 batches | ms/batch 12.36 | loss  1.22 | perplexity     3.40\n",
      "|   600/  900 batches | ms/batch 12.38 | loss  1.22 | perplexity     3.39\n",
      "|   800/  900 batches | ms/batch 12.38 | loss  1.23 | perplexity     3.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.91s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.31 | loss  1.23 | perplexity     3.43\n",
      "|   400/  900 batches | ms/batch 11.61 | loss  1.23 | perplexity     3.43\n",
      "|   600/  900 batches | ms/batch 11.69 | loss  1.22 | perplexity     3.38\n",
      "|   800/  900 batches | ms/batch 11.64 | loss  1.21 | perplexity     3.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.38s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.70 | loss  1.21 | perplexity     3.37\n",
      "|   400/  900 batches | ms/batch 11.83 | loss  1.20 | perplexity     3.30\n",
      "|   600/  900 batches | ms/batch 13.67 | loss  1.18 | perplexity     3.26\n",
      "|   800/  900 batches | ms/batch 13.88 | loss  1.15 | perplexity     3.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 13.31s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.90 | loss  1.10 | perplexity     3.00\n",
      "|   400/  900 batches | ms/batch 12.20 | loss  1.07 | perplexity     2.91\n",
      "|   600/  900 batches | ms/batch 12.30 | loss  1.06 | perplexity     2.89\n",
      "|   800/  900 batches | ms/batch 12.31 | loss  1.03 | perplexity     2.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.12s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.50 | loss  1.02 | perplexity     2.77\n",
      "|   400/  900 batches | ms/batch 12.39 | loss  1.01 | perplexity     2.74\n",
      "|   600/  900 batches | ms/batch 12.33 | loss  0.99 | perplexity     2.69\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  0.98 | perplexity     2.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 12.88s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.48 | loss  0.98 | perplexity     2.67\n",
      "|   400/  900 batches | ms/batch 11.65 | loss  0.97 | perplexity     2.63\n",
      "|   600/  900 batches | ms/batch 11.64 | loss  0.98 | perplexity     2.67\n",
      "|   800/  900 batches | ms/batch 11.65 | loss  0.95 | perplexity     2.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 12.42s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.72 | loss  0.97 | perplexity     2.65\n",
      "|   400/  900 batches | ms/batch 11.62 | loss  0.95 | perplexity     2.59\n",
      "|   600/  900 batches | ms/batch 11.62 | loss  0.96 | perplexity     2.61\n",
      "|   800/  900 batches | ms/batch 11.67 | loss  0.96 | perplexity     2.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.25s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.70 | loss  0.95 | perplexity     2.59\n",
      "|   400/  900 batches | ms/batch 11.64 | loss  0.95 | perplexity     2.59\n",
      "|   600/  900 batches | ms/batch 11.95 | loss  0.93 | perplexity     2.55\n",
      "|   800/  900 batches | ms/batch 12.20 | loss  0.93 | perplexity     2.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 12.50s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.34 | loss  0.94 | perplexity     2.55\n",
      "|   400/  900 batches | ms/batch 11.83 | loss  0.94 | perplexity     2.55\n",
      "|   600/  900 batches | ms/batch 12.20 | loss  0.94 | perplexity     2.57\n",
      "|   800/  900 batches | ms/batch 12.22 | loss  0.93 | perplexity     2.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 12.72s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.41 | loss  0.93 | perplexity     2.54\n",
      "|   400/  900 batches | ms/batch 12.21 | loss  0.92 | perplexity     2.50\n",
      "|   600/  900 batches | ms/batch 12.20 | loss  0.92 | perplexity     2.52\n",
      "|   800/  900 batches | ms/batch 12.21 | loss  0.92 | perplexity     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 12.81s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.53 | loss  0.92 | perplexity     2.50\n",
      "|   400/  900 batches | ms/batch 12.33 | loss  0.91 | perplexity     2.48\n",
      "|   600/  900 batches | ms/batch 12.03 | loss  0.90 | perplexity     2.47\n",
      "|   800/  900 batches | ms/batch 12.49 | loss  0.90 | perplexity     2.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.89s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.40 | loss  0.91 | perplexity     2.48\n",
      "|   400/  900 batches | ms/batch 12.54 | loss  0.90 | perplexity     2.47\n",
      "|   600/  900 batches | ms/batch 12.50 | loss  0.89 | perplexity     2.45\n",
      "|   800/  900 batches | ms/batch 12.44 | loss  0.90 | perplexity     2.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 12.91s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.39 | loss  0.89 | perplexity     2.45\n",
      "|   400/  900 batches | ms/batch 12.37 | loss  0.89 | perplexity     2.43\n",
      "|   600/  900 batches | ms/batch 12.23 | loss  0.89 | perplexity     2.43\n",
      "|   800/  900 batches | ms/batch 12.16 | loss  0.90 | perplexity     2.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.76s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.15 | loss  0.90 | perplexity     2.45\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  0.89 | perplexity     2.44\n",
      "|   600/  900 batches | ms/batch 12.42 | loss  0.89 | perplexity     2.45\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  0.88 | perplexity     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.75s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.47 | loss  0.90 | perplexity     2.45\n",
      "|   400/  900 batches | ms/batch 11.40 | loss  0.90 | perplexity     2.45\n",
      "|   600/  900 batches | ms/batch 11.40 | loss  0.89 | perplexity     2.43\n",
      "|   800/  900 batches | ms/batch 11.40 | loss  0.89 | perplexity     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 11.97s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.48 | loss  0.89 | perplexity     2.43\n",
      "|   400/  900 batches | ms/batch 11.42 | loss  0.88 | perplexity     2.41\n",
      "|   600/  900 batches | ms/batch 12.06 | loss  0.87 | perplexity     2.39\n",
      "|   800/  900 batches | ms/batch 12.14 | loss  0.88 | perplexity     2.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 12.27s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.56 | loss  0.88 | perplexity     2.42\n",
      "|   400/  900 batches | ms/batch 11.54 | loss  0.88 | perplexity     2.40\n",
      "|   600/  900 batches | ms/batch 11.43 | loss  0.87 | perplexity     2.39\n",
      "|   800/  900 batches | ms/batch 11.61 | loss  0.88 | perplexity     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 12.16s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.23 | loss  0.87 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  0.87 | perplexity     2.39\n",
      "|   600/  900 batches | ms/batch 12.21 | loss  0.87 | perplexity     2.39\n",
      "|   800/  900 batches | ms/batch 12.20 | loss  0.87 | perplexity     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 12.69s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.19 | loss  0.87 | perplexity     2.39\n",
      "|   400/  900 batches | ms/batch 11.85 | loss  0.87 | perplexity     2.38\n",
      "|   600/  900 batches | ms/batch 12.13 | loss  0.87 | perplexity     2.38\n",
      "|   800/  900 batches | ms/batch 12.40 | loss  0.87 | perplexity     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 12.69s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.44 | loss  0.87 | perplexity     2.39\n",
      "|   400/  900 batches | ms/batch 12.44 | loss  0.87 | perplexity     2.39\n",
      "|   600/  900 batches | ms/batch 12.37 | loss  0.88 | perplexity     2.41\n",
      "|   800/  900 batches | ms/batch 12.44 | loss  0.86 | perplexity     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 12.90s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.49 | loss  0.88 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 12.55 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 12.46 | loss  0.86 | perplexity     2.37\n",
      "|   800/  900 batches | ms/batch 12.41 | loss  0.87 | perplexity     2.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 12.95s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.48 | loss  0.87 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 12.36 | loss  0.87 | perplexity     2.38\n",
      "|   600/  900 batches | ms/batch 12.36 | loss  0.87 | perplexity     2.38\n",
      "|   800/  900 batches | ms/batch 12.42 | loss  0.86 | perplexity     2.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 12.89s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.46 | loss  0.87 | perplexity     2.38\n",
      "|   400/  900 batches | ms/batch 12.60 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 12.09 | loss  0.86 | perplexity     2.36\n",
      "|   800/  900 batches | ms/batch 12.49 | loss  0.86 | perplexity     2.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 12.88s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.63 | loss  0.86 | perplexity     2.37\n",
      "|   400/  900 batches | ms/batch 11.50 | loss  0.85 | perplexity     2.35\n",
      "|   600/  900 batches | ms/batch 11.97 | loss  0.86 | perplexity     2.36\n",
      "|   800/  900 batches | ms/batch 12.50 | loss  0.87 | perplexity     2.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 12.51s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.48 | loss  0.87 | perplexity     2.38\n",
      "|   400/  900 batches | ms/batch 12.41 | loss  0.85 | perplexity     2.35\n",
      "|   600/  900 batches | ms/batch 12.47 | loss  0.84 | perplexity     2.33\n",
      "|   800/  900 batches | ms/batch 12.36 | loss  0.85 | perplexity     2.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 12.89s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.32 | loss  0.86 | perplexity     2.37\n",
      "|   400/  900 batches | ms/batch 12.17 | loss  0.86 | perplexity     2.36\n",
      "|   600/  900 batches | ms/batch 12.26 | loss  0.86 | perplexity     2.36\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  0.85 | perplexity     2.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 12.77s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.27 | loss  0.86 | perplexity     2.36\n",
      "|   400/  900 batches | ms/batch 12.44 | loss  0.85 | perplexity     2.34\n",
      "|   600/  900 batches | ms/batch 12.33 | loss  0.85 | perplexity     2.34\n",
      "|   800/  900 batches | ms/batch 12.27 | loss  0.86 | perplexity     2.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 12.84s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.51 | loss  0.85 | perplexity     2.34\n",
      "|   400/  900 batches | ms/batch 12.18 | loss  0.84 | perplexity     2.32\n",
      "|   600/  900 batches | ms/batch 12.20 | loss  0.83 | perplexity     2.28\n",
      "|   800/  900 batches | ms/batch 12.26 | loss  0.83 | perplexity     2.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 12.79s | test accuracy  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.30 | loss  0.81 | perplexity     2.26\n",
      "|   400/  900 batches | ms/batch 12.28 | loss  0.81 | perplexity     2.24\n",
      "|   600/  900 batches | ms/batch 12.18 | loss  0.79 | perplexity     2.21\n",
      "|   800/  900 batches | ms/batch 12.24 | loss  0.78 | perplexity     2.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 12.75s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.24 | loss  0.77 | perplexity     2.16\n",
      "|   400/  900 batches | ms/batch 12.22 | loss  0.75 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 12.09 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 11.56 | loss  0.74 | perplexity     2.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 12.48s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.04 | loss  0.73 | perplexity     2.07\n",
      "|   400/  900 batches | ms/batch 12.13 | loss  0.72 | perplexity     2.06\n",
      "|   600/  900 batches | ms/batch 12.26 | loss  0.71 | perplexity     2.03\n",
      "|   800/  900 batches | ms/batch 12.57 | loss  0.71 | perplexity     2.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 12.83s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  0.70 | perplexity     2.02\n",
      "|   400/  900 batches | ms/batch 12.61 | loss  0.69 | perplexity     1.99\n",
      "|   600/  900 batches | ms/batch 12.35 | loss  0.69 | perplexity     2.00\n",
      "|   800/  900 batches | ms/batch 12.13 | loss  0.69 | perplexity     2.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 12.91s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.25 | loss  0.69 | perplexity     1.99\n",
      "|   400/  900 batches | ms/batch 12.25 | loss  0.68 | perplexity     1.97\n",
      "|   600/  900 batches | ms/batch 12.09 | loss  0.68 | perplexity     1.98\n",
      "|   800/  900 batches | ms/batch 12.16 | loss  0.67 | perplexity     1.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 12.70s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.31 | loss  0.67 | perplexity     1.96\n",
      "|   400/  900 batches | ms/batch 12.44 | loss  0.66 | perplexity     1.94\n",
      "|   600/  900 batches | ms/batch 12.62 | loss  0.66 | perplexity     1.94\n",
      "|   800/  900 batches | ms/batch 12.45 | loss  0.66 | perplexity     1.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 12.98s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.49 | loss  0.66 | perplexity     1.94\n",
      "|   400/  900 batches | ms/batch 11.76 | loss  0.64 | perplexity     1.89\n",
      "|   600/  900 batches | ms/batch 11.41 | loss  0.65 | perplexity     1.91\n",
      "|   800/  900 batches | ms/batch 11.41 | loss  0.65 | perplexity     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 12.32s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.53 | loss  0.65 | perplexity     1.92\n",
      "|   400/  900 batches | ms/batch 11.41 | loss  0.64 | perplexity     1.90\n",
      "|   600/  900 batches | ms/batch 11.41 | loss  0.63 | perplexity     1.89\n",
      "|   800/  900 batches | ms/batch 11.42 | loss  0.64 | perplexity     1.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 12.03s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.76 | loss  0.64 | perplexity     1.90\n",
      "|   400/  900 batches | ms/batch 12.87 | loss  0.63 | perplexity     1.88\n",
      "|   600/  900 batches | ms/batch 12.72 | loss  0.63 | perplexity     1.88\n",
      "|   800/  900 batches | ms/batch 12.51 | loss  0.62 | perplexity     1.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 13.15s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.51 | loss  0.63 | perplexity     1.88\n",
      "|   400/  900 batches | ms/batch 12.56 | loss  0.61 | perplexity     1.85\n",
      "|   600/  900 batches | ms/batch 12.55 | loss  0.64 | perplexity     1.89\n",
      "|   800/  900 batches | ms/batch 12.42 | loss  0.63 | perplexity     1.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 13.00s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.76 | loss  0.62 | perplexity     1.86\n",
      "|   400/  900 batches | ms/batch 12.40 | loss  0.61 | perplexity     1.85\n",
      "|   600/  900 batches | ms/batch 11.66 | loss  0.62 | perplexity     1.86\n",
      "|   800/  900 batches | ms/batch 11.62 | loss  0.62 | perplexity     1.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 12.58s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.81 | loss  0.61 | perplexity     1.85\n",
      "|   400/  900 batches | ms/batch 12.06 | loss  0.60 | perplexity     1.83\n",
      "|   600/  900 batches | ms/batch 12.58 | loss  0.60 | perplexity     1.82\n",
      "|   800/  900 batches | ms/batch 12.57 | loss  0.61 | perplexity     1.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 12.88s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.81 | loss  0.60 | perplexity     1.83\n",
      "|   400/  900 batches | ms/batch 12.72 | loss  0.60 | perplexity     1.81\n",
      "|   600/  900 batches | ms/batch 12.71 | loss  0.61 | perplexity     1.84\n",
      "|   800/  900 batches | ms/batch 12.73 | loss  0.60 | perplexity     1.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 13.27s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.79 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 12.68 | loss  0.60 | perplexity     1.83\n",
      "|   600/  900 batches | ms/batch 12.71 | loss  0.59 | perplexity     1.81\n",
      "|   800/  900 batches | ms/batch 12.57 | loss  0.59 | perplexity     1.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 13.23s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.67 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 12.56 | loss  0.58 | perplexity     1.79\n",
      "|   600/  900 batches | ms/batch 12.56 | loss  0.59 | perplexity     1.81\n",
      "|   800/  900 batches | ms/batch 12.29 | loss  0.59 | perplexity     1.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 13.05s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.48 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 12.46 | loss  0.59 | perplexity     1.80\n",
      "|   600/  900 batches | ms/batch 12.73 | loss  0.57 | perplexity     1.77\n",
      "|   800/  900 batches | ms/batch 12.02 | loss  0.57 | perplexity     1.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 12.85s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Training 200 epochs on 5 bits\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.31 | loss  1.96 | perplexity     7.07\n",
      "|   400/  900 batches | ms/batch 12.11 | loss  1.70 | perplexity     5.47\n",
      "|   600/  900 batches | ms/batch 12.18 | loss  1.59 | perplexity     4.92\n",
      "|   800/  900 batches | ms/batch 12.08 | loss  1.54 | perplexity     4.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.96s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.27 | loss  1.50 | perplexity     4.50\n",
      "|   400/  900 batches | ms/batch 12.30 | loss  1.48 | perplexity     4.38\n",
      "|   600/  900 batches | ms/batch 12.01 | loss  1.46 | perplexity     4.31\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  1.47 | perplexity     4.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.61s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.54 | loss  1.45 | perplexity     4.28\n",
      "|   400/  900 batches | ms/batch 12.27 | loss  1.44 | perplexity     4.23\n",
      "|   600/  900 batches | ms/batch 12.24 | loss  1.43 | perplexity     4.19\n",
      "|   800/  900 batches | ms/batch 12.27 | loss  1.43 | perplexity     4.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.29s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.41 | loss  1.43 | perplexity     4.20\n",
      "|   400/  900 batches | ms/batch 12.28 | loss  1.42 | perplexity     4.14\n",
      "|   600/  900 batches | ms/batch 12.35 | loss  1.41 | perplexity     4.12\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  1.42 | perplexity     4.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.25s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.10 | loss  1.42 | perplexity     4.14\n",
      "|   400/  900 batches | ms/batch 11.99 | loss  1.41 | perplexity     4.09\n",
      "|   600/  900 batches | ms/batch 12.53 | loss  1.40 | perplexity     4.07\n",
      "|   800/  900 batches | ms/batch 12.32 | loss  1.40 | perplexity     4.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.00s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.03 | loss  1.40 | perplexity     4.06\n",
      "|   400/  900 batches | ms/batch 12.59 | loss  1.40 | perplexity     4.06\n",
      "|   600/  900 batches | ms/batch 11.96 | loss  1.39 | perplexity     4.03\n",
      "|   800/  900 batches | ms/batch 11.93 | loss  1.39 | perplexity     4.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.96s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.66 | loss  1.39 | perplexity     4.03\n",
      "|   400/  900 batches | ms/batch 12.69 | loss  1.39 | perplexity     4.03\n",
      "|   600/  900 batches | ms/batch 12.73 | loss  1.39 | perplexity     4.01\n",
      "|   800/  900 batches | ms/batch 12.75 | loss  1.40 | perplexity     4.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 13.50s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.21 | loss  1.40 | perplexity     4.04\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  1.39 | perplexity     4.00\n",
      "|   600/  900 batches | ms/batch 12.55 | loss  1.38 | perplexity     3.96\n",
      "|   800/  900 batches | ms/batch 12.28 | loss  1.38 | perplexity     3.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 13.15s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.51 | loss  1.38 | perplexity     3.99\n",
      "|   400/  900 batches | ms/batch 12.35 | loss  1.38 | perplexity     3.98\n",
      "|   600/  900 batches | ms/batch 12.64 | loss  1.38 | perplexity     3.96\n",
      "|   800/  900 batches | ms/batch 12.40 | loss  1.38 | perplexity     3.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 13.30s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.01 | loss  1.39 | perplexity     4.00\n",
      "|   400/  900 batches | ms/batch 11.94 | loss  1.38 | perplexity     3.97\n",
      "|   600/  900 batches | ms/batch 12.24 | loss  1.39 | perplexity     4.01\n",
      "|   800/  900 batches | ms/batch 12.79 | loss  1.36 | perplexity     3.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.17s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.88 | loss  1.35 | perplexity     3.85\n",
      "|   400/  900 batches | ms/batch 12.80 | loss  1.32 | perplexity     3.75\n",
      "|   600/  900 batches | ms/batch 12.79 | loss  1.31 | perplexity     3.70\n",
      "|   800/  900 batches | ms/batch 12.80 | loss  1.31 | perplexity     3.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 13.58s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.72 | loss  1.24 | perplexity     3.46\n",
      "|   400/  900 batches | ms/batch 12.83 | loss  1.22 | perplexity     3.39\n",
      "|   600/  900 batches | ms/batch 12.29 | loss  1.22 | perplexity     3.40\n",
      "|   800/  900 batches | ms/batch 11.68 | loss  1.20 | perplexity     3.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 13.09s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.21 | loss  1.20 | perplexity     3.32\n",
      "|   400/  900 batches | ms/batch 12.63 | loss  1.18 | perplexity     3.25\n",
      "|   600/  900 batches | ms/batch 12.41 | loss  1.18 | perplexity     3.26\n",
      "|   800/  900 batches | ms/batch 12.53 | loss  1.17 | perplexity     3.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 13.17s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.20 | loss  1.17 | perplexity     3.24\n",
      "|   400/  900 batches | ms/batch 12.68 | loss  1.16 | perplexity     3.18\n",
      "|   600/  900 batches | ms/batch 12.76 | loss  1.15 | perplexity     3.17\n",
      "|   800/  900 batches | ms/batch 12.87 | loss  1.15 | perplexity     3.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 13.36s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.30 | loss  1.15 | perplexity     3.17\n",
      "|   400/  900 batches | ms/batch 12.29 | loss  1.15 | perplexity     3.16\n",
      "|   600/  900 batches | ms/batch 12.30 | loss  1.14 | perplexity     3.14\n",
      "|   800/  900 batches | ms/batch 12.41 | loss  1.15 | perplexity     3.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 13.12s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.38 | loss  1.15 | perplexity     3.15\n",
      "|   400/  900 batches | ms/batch 12.19 | loss  1.14 | perplexity     3.13\n",
      "|   600/  900 batches | ms/batch 12.25 | loss  1.14 | perplexity     3.13\n",
      "|   800/  900 batches | ms/batch 12.16 | loss  1.13 | perplexity     3.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 12.94s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.82 | loss  1.16 | perplexity     3.18\n",
      "|   400/  900 batches | ms/batch 12.00 | loss  1.15 | perplexity     3.17\n",
      "|   600/  900 batches | ms/batch 11.48 | loss  1.14 | perplexity     3.11\n",
      "|   800/  900 batches | ms/batch 11.49 | loss  1.14 | perplexity     3.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.48s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.60 | loss  1.13 | perplexity     3.10\n",
      "|   400/  900 batches | ms/batch 11.50 | loss  1.12 | perplexity     3.07\n",
      "|   600/  900 batches | ms/batch 11.49 | loss  1.12 | perplexity     3.06\n",
      "|   800/  900 batches | ms/batch 11.49 | loss  1.12 | perplexity     3.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 12.34s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.55 | loss  1.12 | perplexity     3.05\n",
      "|   400/  900 batches | ms/batch 11.48 | loss  1.11 | perplexity     3.05\n",
      "|   600/  900 batches | ms/batch 11.57 | loss  1.11 | perplexity     3.04\n",
      "|   800/  900 batches | ms/batch 12.16 | loss  1.11 | perplexity     3.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.58s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.57 | loss  1.11 | perplexity     3.03\n",
      "|   400/  900 batches | ms/batch 12.20 | loss  1.10 | perplexity     3.01\n",
      "|   600/  900 batches | ms/batch 11.48 | loss  1.10 | perplexity     3.00\n",
      "|   800/  900 batches | ms/batch 11.47 | loss  1.09 | perplexity     2.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.78s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.07 | loss  1.10 | perplexity     3.01\n",
      "|   400/  900 batches | ms/batch 12.32 | loss  1.09 | perplexity     2.98\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  1.09 | perplexity     2.97\n",
      "|   800/  900 batches | ms/batch 12.36 | loss  1.10 | perplexity     2.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 12.96s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.83 | loss  1.09 | perplexity     2.99\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  1.09 | perplexity     2.96\n",
      "|   600/  900 batches | ms/batch 12.20 | loss  1.08 | perplexity     2.96\n",
      "|   800/  900 batches | ms/batch 11.81 | loss  1.08 | perplexity     2.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 12.71s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.55 | loss  1.09 | perplexity     2.96\n",
      "|   400/  900 batches | ms/batch 12.64 | loss  1.08 | perplexity     2.95\n",
      "|   600/  900 batches | ms/batch 12.14 | loss  1.09 | perplexity     2.96\n",
      "|   800/  900 batches | ms/batch 11.98 | loss  1.08 | perplexity     2.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 12.93s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.01 | loss  1.09 | perplexity     2.97\n",
      "|   400/  900 batches | ms/batch 12.17 | loss  1.08 | perplexity     2.94\n",
      "|   600/  900 batches | ms/batch 12.29 | loss  1.08 | perplexity     2.94\n",
      "|   800/  900 batches | ms/batch 11.91 | loss  1.08 | perplexity     2.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 12.80s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.53 | loss  1.08 | perplexity     2.95\n",
      "|   400/  900 batches | ms/batch 11.97 | loss  1.07 | perplexity     2.92\n",
      "|   600/  900 batches | ms/batch 12.69 | loss  1.08 | perplexity     2.93\n",
      "|   800/  900 batches | ms/batch 11.62 | loss  1.08 | perplexity     2.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 12.68s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.53 | loss  1.08 | perplexity     2.94\n",
      "|   400/  900 batches | ms/batch 11.47 | loss  1.07 | perplexity     2.91\n",
      "|   600/  900 batches | ms/batch 11.75 | loss  1.08 | perplexity     2.93\n",
      "|   800/  900 batches | ms/batch 11.47 | loss  1.07 | perplexity     2.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 12.36s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.53 | loss  1.08 | perplexity     2.94\n",
      "|   400/  900 batches | ms/batch 11.46 | loss  1.07 | perplexity     2.92\n",
      "|   600/  900 batches | ms/batch 11.46 | loss  1.07 | perplexity     2.92\n",
      "|   800/  900 batches | ms/batch 11.46 | loss  1.06 | perplexity     2.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 12.35s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.54 | loss  1.07 | perplexity     2.91\n",
      "|   400/  900 batches | ms/batch 11.52 | loss  1.06 | perplexity     2.90\n",
      "|   600/  900 batches | ms/batch 12.20 | loss  1.07 | perplexity     2.90\n",
      "|   800/  900 batches | ms/batch 11.84 | loss  1.07 | perplexity     2.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 12.58s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.97 | loss  1.06 | perplexity     2.89\n",
      "|   400/  900 batches | ms/batch 12.07 | loss  1.05 | perplexity     2.85\n",
      "|   600/  900 batches | ms/batch 12.48 | loss  1.04 | perplexity     2.82\n",
      "|   800/  900 batches | ms/batch 12.33 | loss  1.04 | perplexity     2.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 13.09s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.80 | loss  1.03 | perplexity     2.80\n",
      "|   400/  900 batches | ms/batch 12.63 | loss  1.03 | perplexity     2.80\n",
      "|   600/  900 batches | ms/batch 12.65 | loss  1.03 | perplexity     2.80\n",
      "|   800/  900 batches | ms/batch 12.64 | loss  1.02 | perplexity     2.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 13.48s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  1.02 | perplexity     2.78\n",
      "|   400/  900 batches | ms/batch 12.70 | loss  1.01 | perplexity     2.75\n",
      "|   600/  900 batches | ms/batch 12.64 | loss  1.01 | perplexity     2.75\n",
      "|   800/  900 batches | ms/batch 12.38 | loss  1.00 | perplexity     2.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 13.30s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.56 | loss  1.00 | perplexity     2.71\n",
      "|   400/  900 batches | ms/batch 12.66 | loss  0.98 | perplexity     2.68\n",
      "|   600/  900 batches | ms/batch 12.54 | loss  0.99 | perplexity     2.68\n",
      "|   800/  900 batches | ms/batch 12.36 | loss  0.98 | perplexity     2.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 13.26s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.48 | loss  0.98 | perplexity     2.66\n",
      "|   400/  900 batches | ms/batch 12.37 | loss  0.96 | perplexity     2.62\n",
      "|   600/  900 batches | ms/batch 12.41 | loss  0.95 | perplexity     2.58\n",
      "|   800/  900 batches | ms/batch 12.44 | loss  0.93 | perplexity     2.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 13.15s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.49 | loss  0.92 | perplexity     2.52\n",
      "|   400/  900 batches | ms/batch 12.39 | loss  0.90 | perplexity     2.46\n",
      "|   600/  900 batches | ms/batch 12.56 | loss  0.90 | perplexity     2.47\n",
      "|   800/  900 batches | ms/batch 12.64 | loss  0.92 | perplexity     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 13.25s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.70 | loss  0.91 | perplexity     2.47\n",
      "|   400/  900 batches | ms/batch 12.65 | loss  0.89 | perplexity     2.43\n",
      "|   600/  900 batches | ms/batch 12.69 | loss  0.89 | perplexity     2.44\n",
      "|   800/  900 batches | ms/batch 12.62 | loss  0.88 | perplexity     2.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 13.37s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.71 | loss  0.89 | perplexity     2.43\n",
      "|   400/  900 batches | ms/batch 12.60 | loss  0.87 | perplexity     2.39\n",
      "|   600/  900 batches | ms/batch 12.75 | loss  0.87 | perplexity     2.39\n",
      "|   800/  900 batches | ms/batch 12.70 | loss  0.88 | perplexity     2.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 13.51s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.82 | loss  0.88 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 12.63 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 12.65 | loss  0.87 | perplexity     2.39\n",
      "|   800/  900 batches | ms/batch 12.65 | loss  0.89 | perplexity     2.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 13.40s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.26 | loss  0.88 | perplexity     2.40\n",
      "|   400/  900 batches | ms/batch 11.98 | loss  0.88 | perplexity     2.42\n",
      "|   600/  900 batches | ms/batch 11.98 | loss  0.87 | perplexity     2.40\n",
      "|   800/  900 batches | ms/batch 11.98 | loss  0.86 | perplexity     2.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 12.91s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.08 | loss  0.87 | perplexity     2.38\n",
      "|   400/  900 batches | ms/batch 12.21 | loss  0.85 | perplexity     2.35\n",
      "|   600/  900 batches | ms/batch 12.55 | loss  0.86 | perplexity     2.37\n",
      "|   800/  900 batches | ms/batch 12.51 | loss  0.87 | perplexity     2.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 13.10s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.38 | loss  0.86 | perplexity     2.36\n",
      "|   400/  900 batches | ms/batch 12.51 | loss  0.86 | perplexity     2.37\n",
      "|   600/  900 batches | ms/batch 12.43 | loss  0.86 | perplexity     2.37\n",
      "|   800/  900 batches | ms/batch 12.38 | loss  0.87 | perplexity     2.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 13.25s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.16 | loss  0.86 | perplexity     2.35\n",
      "|   400/  900 batches | ms/batch 12.08 | loss  0.84 | perplexity     2.33\n",
      "|   600/  900 batches | ms/batch 11.99 | loss  0.85 | perplexity     2.35\n",
      "|   800/  900 batches | ms/batch 12.31 | loss  0.85 | perplexity     2.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 13.00s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.72 | loss  0.85 | perplexity     2.34\n",
      "|   400/  900 batches | ms/batch 12.40 | loss  0.84 | perplexity     2.33\n",
      "|   600/  900 batches | ms/batch 12.46 | loss  0.84 | perplexity     2.31\n",
      "|   800/  900 batches | ms/batch 12.49 | loss  0.84 | perplexity     2.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 13.23s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.53 | loss  0.84 | perplexity     2.33\n",
      "|   400/  900 batches | ms/batch 12.37 | loss  0.83 | perplexity     2.30\n",
      "|   600/  900 batches | ms/batch 12.66 | loss  0.84 | perplexity     2.31\n",
      "|   800/  900 batches | ms/batch 12.64 | loss  0.83 | perplexity     2.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 13.40s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.05 | loss  0.84 | perplexity     2.32\n",
      "|   400/  900 batches | ms/batch 12.72 | loss  0.83 | perplexity     2.30\n",
      "|   600/  900 batches | ms/batch 12.76 | loss  0.83 | perplexity     2.30\n",
      "|   800/  900 batches | ms/batch 12.71 | loss  0.83 | perplexity     2.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 13.59s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.17 | loss  0.83 | perplexity     2.30\n",
      "|   400/  900 batches | ms/batch 12.33 | loss  0.82 | perplexity     2.28\n",
      "|   600/  900 batches | ms/batch 12.60 | loss  0.84 | perplexity     2.33\n",
      "|   800/  900 batches | ms/batch 12.67 | loss  0.84 | perplexity     2.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 13.30s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.62 | loss  0.84 | perplexity     2.32\n",
      "|   400/  900 batches | ms/batch 12.47 | loss  0.83 | perplexity     2.30\n",
      "|   600/  900 batches | ms/batch 12.41 | loss  0.83 | perplexity     2.29\n",
      "|   800/  900 batches | ms/batch 12.42 | loss  0.82 | perplexity     2.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 13.19s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.18 | loss  0.83 | perplexity     2.30\n",
      "|   400/  900 batches | ms/batch 12.09 | loss  0.82 | perplexity     2.28\n",
      "|   600/  900 batches | ms/batch 12.16 | loss  0.83 | perplexity     2.28\n",
      "|   800/  900 batches | ms/batch 12.61 | loss  0.82 | perplexity     2.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 13.27s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.62 | loss  0.83 | perplexity     2.29\n",
      "|   400/  900 batches | ms/batch 12.90 | loss  0.82 | perplexity     2.27\n",
      "|   600/  900 batches | ms/batch 12.88 | loss  0.82 | perplexity     2.26\n",
      "|   800/  900 batches | ms/batch 12.73 | loss  0.82 | perplexity     2.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 13.46s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.67 | loss  0.82 | perplexity     2.27\n",
      "|   400/  900 batches | ms/batch 12.70 | loss  0.82 | perplexity     2.27\n",
      "|   600/  900 batches | ms/batch 12.80 | loss  0.82 | perplexity     2.26\n",
      "|   800/  900 batches | ms/batch 13.31 | loss  0.82 | perplexity     2.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 13.64s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.99 | loss  0.82 | perplexity     2.28\n",
      "|   400/  900 batches | ms/batch 11.56 | loss  0.82 | perplexity     2.27\n",
      "|   600/  900 batches | ms/batch 11.57 | loss  0.82 | perplexity     2.26\n",
      "|   800/  900 batches | ms/batch 11.67 | loss  0.81 | perplexity     2.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 12.55s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.55 | loss  0.82 | perplexity     2.27\n",
      "|   400/  900 batches | ms/batch 11.53 | loss  0.81 | perplexity     2.25\n",
      "|   600/  900 batches | ms/batch 12.12 | loss  0.81 | perplexity     2.25\n",
      "|   800/  900 batches | ms/batch 11.60 | loss  0.83 | perplexity     2.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 12.57s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.66 | loss  0.82 | perplexity     2.28\n",
      "|   400/  900 batches | ms/batch 11.58 | loss  0.81 | perplexity     2.24\n",
      "|   600/  900 batches | ms/batch 11.58 | loss  0.82 | perplexity     2.26\n",
      "|   800/  900 batches | ms/batch 11.59 | loss  0.80 | perplexity     2.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 12.44s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.29 | loss  0.82 | perplexity     2.27\n",
      "|   400/  900 batches | ms/batch 12.24 | loss  0.81 | perplexity     2.25\n",
      "|   600/  900 batches | ms/batch 12.18 | loss  0.82 | perplexity     2.26\n",
      "|   800/  900 batches | ms/batch 12.19 | loss  0.82 | perplexity     2.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 13.00s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.31 | loss  0.81 | perplexity     2.25\n",
      "|   400/  900 batches | ms/batch 12.08 | loss  0.81 | perplexity     2.24\n",
      "|   600/  900 batches | ms/batch 11.71 | loss  0.81 | perplexity     2.25\n",
      "|   800/  900 batches | ms/batch 11.79 | loss  0.81 | perplexity     2.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 12.77s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.77 | loss  0.81 | perplexity     2.25\n",
      "|   400/  900 batches | ms/batch 11.71 | loss  0.82 | perplexity     2.26\n",
      "|   600/  900 batches | ms/batch 11.70 | loss  0.81 | perplexity     2.25\n",
      "|   800/  900 batches | ms/batch 11.71 | loss  0.81 | perplexity     2.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 12.57s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.87 | loss  0.82 | perplexity     2.26\n",
      "|   400/  900 batches | ms/batch 12.00 | loss  0.81 | perplexity     2.25\n",
      "|   600/  900 batches | ms/batch 12.22 | loss  0.80 | perplexity     2.23\n",
      "|   800/  900 batches | ms/batch 12.03 | loss  0.80 | perplexity     2.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 12.81s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.43 | loss  0.81 | perplexity     2.25\n",
      "|   400/  900 batches | ms/batch 12.28 | loss  0.81 | perplexity     2.25\n",
      "|   600/  900 batches | ms/batch 12.50 | loss  0.82 | perplexity     2.27\n",
      "|   800/  900 batches | ms/batch 11.81 | loss  0.80 | perplexity     2.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 13.00s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.32 | loss  0.81 | perplexity     2.25\n",
      "|   400/  900 batches | ms/batch 12.49 | loss  0.81 | perplexity     2.24\n",
      "|   600/  900 batches | ms/batch 12.42 | loss  0.81 | perplexity     2.24\n",
      "|   800/  900 batches | ms/batch 12.21 | loss  0.84 | perplexity     2.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 13.14s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.44 | loss  0.82 | perplexity     2.26\n",
      "|   400/  900 batches | ms/batch 12.71 | loss  0.81 | perplexity     2.25\n",
      "|   600/  900 batches | ms/batch 12.77 | loss  0.80 | perplexity     2.23\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  0.80 | perplexity     2.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 13.60s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.68 | loss  0.81 | perplexity     2.24\n",
      "|   400/  900 batches | ms/batch 12.90 | loss  0.82 | perplexity     2.28\n",
      "|   600/  900 batches | ms/batch 12.65 | loss  0.80 | perplexity     2.24\n",
      "|   800/  900 batches | ms/batch 12.79 | loss  0.80 | perplexity     2.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 13.48s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.64 | loss  0.82 | perplexity     2.26\n",
      "|   400/  900 batches | ms/batch 11.79 | loss  0.81 | perplexity     2.24\n",
      "|   600/  900 batches | ms/batch 11.71 | loss  0.80 | perplexity     2.23\n",
      "|   800/  900 batches | ms/batch 11.69 | loss  0.80 | perplexity     2.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 12.76s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.94 | loss  0.81 | perplexity     2.24\n",
      "|   400/  900 batches | ms/batch 12.03 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 11.99 | loss  0.81 | perplexity     2.25\n",
      "|   800/  900 batches | ms/batch 12.16 | loss  0.80 | perplexity     2.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 12.96s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.11 | loss  0.81 | perplexity     2.24\n",
      "|   400/  900 batches | ms/batch 12.89 | loss  0.80 | perplexity     2.21\n",
      "|   600/  900 batches | ms/batch 12.58 | loss  0.80 | perplexity     2.22\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  0.80 | perplexity     2.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 13.56s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.28 | loss  0.80 | perplexity     2.22\n",
      "|   400/  900 batches | ms/batch 12.21 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 12.17 | loss  0.79 | perplexity     2.21\n",
      "|   800/  900 batches | ms/batch 12.81 | loss  0.79 | perplexity     2.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 13.20s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.38 | loss  0.80 | perplexity     2.22\n",
      "|   400/  900 batches | ms/batch 12.16 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 12.27 | loss  0.80 | perplexity     2.24\n",
      "|   800/  900 batches | ms/batch 12.53 | loss  0.79 | perplexity     2.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 13.15s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.71 | loss  0.80 | perplexity     2.23\n",
      "|   400/  900 batches | ms/batch 12.50 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 12.49 | loss  0.80 | perplexity     2.22\n",
      "|   800/  900 batches | ms/batch 12.17 | loss  0.80 | perplexity     2.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 13.30s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.93 | loss  0.80 | perplexity     2.22\n",
      "|   400/  900 batches | ms/batch 12.33 | loss  0.79 | perplexity     2.21\n",
      "|   600/  900 batches | ms/batch 12.48 | loss  0.80 | perplexity     2.22\n",
      "|   800/  900 batches | ms/batch 12.42 | loss  0.80 | perplexity     2.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 13.43s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.81 | loss  0.80 | perplexity     2.23\n",
      "|   400/  900 batches | ms/batch 12.82 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 12.81 | loss  0.79 | perplexity     2.21\n",
      "|   800/  900 batches | ms/batch 12.46 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 13.45s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.23 | loss  0.80 | perplexity     2.22\n",
      "|   400/  900 batches | ms/batch 11.71 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 11.70 | loss  0.79 | perplexity     2.21\n",
      "|   800/  900 batches | ms/batch 11.70 | loss  0.79 | perplexity     2.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 12.66s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.20 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 11.78 | loss  0.79 | perplexity     2.21\n",
      "|   600/  900 batches | ms/batch 12.78 | loss  0.78 | perplexity     2.19\n",
      "|   800/  900 batches | ms/batch 12.68 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 13.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.66 | loss  0.80 | perplexity     2.22\n",
      "|   400/  900 batches | ms/batch 12.64 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 12.16 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.56 | loss  0.82 | perplexity     2.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 13.26s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.58 | loss  0.80 | perplexity     2.23\n",
      "|   400/  900 batches | ms/batch 12.56 | loss  0.79 | perplexity     2.21\n",
      "|   600/  900 batches | ms/batch 12.69 | loss  0.79 | perplexity     2.20\n",
      "|   800/  900 batches | ms/batch 12.54 | loss  0.79 | perplexity     2.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 13.38s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.80 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 12.97 | loss  0.80 | perplexity     2.22\n",
      "|   600/  900 batches | ms/batch 13.04 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.78 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 13.65s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.16 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 12.75 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 12.41 | loss  0.79 | perplexity     2.20\n",
      "|   800/  900 batches | ms/batch 11.99 | loss  0.80 | perplexity     2.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 13.23s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.78 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 11.85 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 11.60 | loss  0.80 | perplexity     2.22\n",
      "|   800/  900 batches | ms/batch 11.58 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 12.53s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.66 | loss  0.79 | perplexity     2.20\n",
      "|   400/  900 batches | ms/batch 11.93 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 12.68 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.27 | loss  0.79 | perplexity     2.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 12.99s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.55 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 12.42 | loss  0.79 | perplexity     2.21\n",
      "|   600/  900 batches | ms/batch 12.51 | loss  0.78 | perplexity     2.19\n",
      "|   800/  900 batches | ms/batch 12.68 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 13.35s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.35 | loss  0.79 | perplexity     2.21\n",
      "|   400/  900 batches | ms/batch 12.74 | loss  0.78 | perplexity     2.18\n",
      "|   600/  900 batches | ms/batch 12.86 | loss  0.78 | perplexity     2.19\n",
      "|   800/  900 batches | ms/batch 12.76 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 13.38s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.44 | loss  0.79 | perplexity     2.20\n",
      "|   400/  900 batches | ms/batch 12.42 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 12.43 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.62 | loss  0.78 | perplexity     2.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 13.32s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.29 | loss  0.78 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.93 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 12.61 | loss  0.78 | perplexity     2.17\n",
      "|   800/  900 batches | ms/batch 12.19 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 13.40s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.94 | loss  0.78 | perplexity     2.18\n",
      "|   400/  900 batches | ms/batch 12.87 | loss  0.78 | perplexity     2.18\n",
      "|   600/  900 batches | ms/batch 12.87 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.82 | loss  0.79 | perplexity     2.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 13.70s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.97 | loss  0.79 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.94 | loss  0.78 | perplexity     2.18\n",
      "|   600/  900 batches | ms/batch 12.97 | loss  0.79 | perplexity     2.21\n",
      "|   800/  900 batches | ms/batch 13.40 | loss  0.79 | perplexity     2.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 13.83s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.40 | loss  0.79 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.59 | loss  0.79 | perplexity     2.20\n",
      "|   600/  900 batches | ms/batch 12.82 | loss  0.80 | perplexity     2.22\n",
      "|   800/  900 batches | ms/batch 13.19 | loss  0.78 | perplexity     2.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 13.59s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.30 | loss  0.79 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.52 | loss  0.78 | perplexity     2.18\n",
      "|   600/  900 batches | ms/batch 11.93 | loss  0.78 | perplexity     2.19\n",
      "|   800/  900 batches | ms/batch 11.96 | loss  0.78 | perplexity     2.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 13.16s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.43 | loss  0.78 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 13.12 | loss  0.78 | perplexity     2.19\n",
      "|   600/  900 batches | ms/batch 12.84 | loss  0.78 | perplexity     2.19\n",
      "|   800/  900 batches | ms/batch 12.42 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 13.42s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.44 | loss  0.79 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.20 | loss  0.77 | perplexity     2.17\n",
      "|   600/  900 batches | ms/batch 12.62 | loss  0.78 | perplexity     2.17\n",
      "|   800/  900 batches | ms/batch 12.07 | loss  0.78 | perplexity     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 13.07s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.20 | loss  0.78 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.30 | loss  0.77 | perplexity     2.16\n",
      "|   600/  900 batches | ms/batch 12.69 | loss  0.77 | perplexity     2.16\n",
      "|   800/  900 batches | ms/batch 13.15 | loss  0.76 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 13.65s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.07 | loss  0.78 | perplexity     2.18\n",
      "|   400/  900 batches | ms/batch 12.79 | loss  0.77 | perplexity     2.17\n",
      "|   600/  900 batches | ms/batch 12.15 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.59 | loss  0.77 | perplexity     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 13.32s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.10 | loss  0.78 | perplexity     2.18\n",
      "|   400/  900 batches | ms/batch 11.80 | loss  0.77 | perplexity     2.15\n",
      "|   600/  900 batches | ms/batch 11.67 | loss  0.80 | perplexity     2.24\n",
      "|   800/  900 batches | ms/batch 11.57 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 12.60s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.83 | loss  0.78 | perplexity     2.19\n",
      "|   400/  900 batches | ms/batch 12.49 | loss  0.78 | perplexity     2.19\n",
      "|   600/  900 batches | ms/batch 13.17 | loss  0.77 | perplexity     2.17\n",
      "|   800/  900 batches | ms/batch 13.44 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 13.54s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.74 | loss  0.78 | perplexity     2.18\n",
      "|   400/  900 batches | ms/batch 12.97 | loss  0.77 | perplexity     2.16\n",
      "|   600/  900 batches | ms/batch 12.73 | loss  0.78 | perplexity     2.18\n",
      "|   800/  900 batches | ms/batch 12.53 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 13.42s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.55 | loss  0.78 | perplexity     2.17\n",
      "|   400/  900 batches | ms/batch 12.65 | loss  0.77 | perplexity     2.16\n",
      "|   600/  900 batches | ms/batch 12.56 | loss  0.76 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 12.62 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 13.32s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  0.77 | perplexity     2.17\n",
      "|   400/  900 batches | ms/batch 12.77 | loss  0.77 | perplexity     2.17\n",
      "|   600/  900 batches | ms/batch 12.73 | loss  0.79 | perplexity     2.20\n",
      "|   800/  900 batches | ms/batch 12.95 | loss  0.77 | perplexity     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 13.51s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.57 | loss  0.77 | perplexity     2.17\n",
      "|   400/  900 batches | ms/batch 12.77 | loss  0.78 | perplexity     2.17\n",
      "|   600/  900 batches | ms/batch 12.92 | loss  0.78 | perplexity     2.17\n",
      "|   800/  900 batches | ms/batch 12.86 | loss  0.78 | perplexity     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 13.63s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.05 | loss  0.77 | perplexity     2.17\n",
      "|   400/  900 batches | ms/batch 12.24 | loss  0.77 | perplexity     2.17\n",
      "|   600/  900 batches | ms/batch 12.56 | loss  0.77 | perplexity     2.16\n",
      "|   800/  900 batches | ms/batch 12.73 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 13.32s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.41 | loss  0.77 | perplexity     2.17\n",
      "|   400/  900 batches | ms/batch 12.38 | loss  0.77 | perplexity     2.15\n",
      "|   600/  900 batches | ms/batch 12.35 | loss  0.77 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 11.81 | loss  0.77 | perplexity     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 12.96s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.83 | loss  0.77 | perplexity     2.16\n",
      "|   400/  900 batches | ms/batch 12.68 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 11.66 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 11.66 | loss  0.77 | perplexity     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 12.73s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.98 | loss  0.77 | perplexity     2.16\n",
      "|   400/  900 batches | ms/batch 12.97 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 11.74 | loss  0.76 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 11.66 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 12.84s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.71 | loss  0.78 | perplexity     2.18\n",
      "|   400/  900 batches | ms/batch 13.06 | loss  0.76 | perplexity     2.14\n",
      "|   600/  900 batches | ms/batch 13.20 | loss  0.77 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 12.93 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 13.79s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.51 | loss  0.77 | perplexity     2.16\n",
      "|   400/  900 batches | ms/batch 11.88 | loss  0.76 | perplexity     2.14\n",
      "|   600/  900 batches | ms/batch 12.37 | loss  0.77 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 12.41 | loss  0.77 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 13.13s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.39 | loss  0.77 | perplexity     2.15\n",
      "|   400/  900 batches | ms/batch 11.96 | loss  0.76 | perplexity     2.14\n",
      "|   600/  900 batches | ms/batch 11.78 | loss  0.76 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 12.18 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 101 | time: 12.97s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.79 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 12.04 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 11.77 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 12.22 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 102 | time: 13.07s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  0.77 | perplexity     2.15\n",
      "|   400/  900 batches | ms/batch 12.58 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 12.61 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 12.61 | loss  0.78 | perplexity     2.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 103 | time: 13.38s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.28 | loss  0.77 | perplexity     2.15\n",
      "|   400/  900 batches | ms/batch 11.95 | loss  0.77 | perplexity     2.16\n",
      "|   600/  900 batches | ms/batch 11.86 | loss  0.76 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 11.77 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 104 | time: 12.77s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.20 | loss  0.76 | perplexity     2.15\n",
      "|   400/  900 batches | ms/batch 11.93 | loss  0.76 | perplexity     2.15\n",
      "|   600/  900 batches | ms/batch 11.79 | loss  0.76 | perplexity     2.15\n",
      "|   800/  900 batches | ms/batch 11.80 | loss  0.75 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 105 | time: 12.74s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.85 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 11.80 | loss  0.75 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 11.79 | loss  0.76 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 11.79 | loss  0.77 | perplexity     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 106 | time: 12.64s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.84 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 11.82 | loss  0.76 | perplexity     2.14\n",
      "|   600/  900 batches | ms/batch 11.77 | loss  0.75 | perplexity     2.12\n",
      "|   800/  900 batches | ms/batch 11.78 | loss  0.76 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 107 | time: 12.77s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.88 | loss  0.77 | perplexity     2.16\n",
      "|   400/  900 batches | ms/batch 12.80 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.84 | loss  0.75 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 12.81 | loss  0.77 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 108 | time: 13.57s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.61 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 12.24 | loss  0.78 | perplexity     2.18\n",
      "|   600/  900 batches | ms/batch 12.28 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 12.27 | loss  0.75 | perplexity     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 109 | time: 13.22s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.32 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.25 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 12.25 | loss  0.75 | perplexity     2.12\n",
      "|   800/  900 batches | ms/batch 12.25 | loss  0.77 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 110 | time: 13.16s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.33 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 12.35 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 12.53 | loss  0.75 | perplexity     2.12\n",
      "|   800/  900 batches | ms/batch 12.61 | loss  0.75 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 111 | time: 13.22s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.66 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 12.57 | loss  0.75 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 12.64 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.59 | loss  0.74 | perplexity     2.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 112 | time: 13.40s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.91 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.78 | loss  0.76 | perplexity     2.14\n",
      "|   600/  900 batches | ms/batch 12.38 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 12.44 | loss  0.76 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 113 | time: 13.46s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.33 | loss  0.77 | perplexity     2.15\n",
      "|   400/  900 batches | ms/batch 12.24 | loss  0.76 | perplexity     2.15\n",
      "|   600/  900 batches | ms/batch 12.35 | loss  0.76 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 12.36 | loss  0.75 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 114 | time: 13.29s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.51 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 12.29 | loss  0.75 | perplexity     2.11\n",
      "|   600/  900 batches | ms/batch 12.45 | loss  0.76 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 12.82 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 115 | time: 13.40s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.36 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.71 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 13.87 | loss  0.75 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 12.96 | loss  0.75 | perplexity     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 116 | time: 13.93s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.69 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 11.85 | loss  0.77 | perplexity     2.15\n",
      "|   600/  900 batches | ms/batch 11.74 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 11.81 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 117 | time: 12.80s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.79 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.16 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 11.70 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 12.69 | loss  0.75 | perplexity     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 118 | time: 12.92s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.58 | loss  0.76 | perplexity     2.14\n",
      "|   400/  900 batches | ms/batch 12.35 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 12.38 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.49 | loss  0.74 | perplexity     2.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 119 | time: 13.15s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.28 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 11.55 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 11.53 | loss  0.75 | perplexity     2.12\n",
      "|   800/  900 batches | ms/batch 12.30 | loss  0.75 | perplexity     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 120 | time: 12.72s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.22 | loss  0.75 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 12.16 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.25 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.34 | loss  0.75 | perplexity     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 121 | time: 13.01s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.49 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.47 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.81 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.83 | loss  0.76 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 122 | time: 13.36s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.57 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.57 | loss  0.74 | perplexity     2.11\n",
      "|   600/  900 batches | ms/batch 12.52 | loss  0.74 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  0.75 | perplexity     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 123 | time: 13.29s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.71 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 12.23 | loss  0.75 | perplexity     2.11\n",
      "|   600/  900 batches | ms/batch 11.66 | loss  0.76 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 11.69 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 124 | time: 12.63s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.73 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 11.98 | loss  0.75 | perplexity     2.12\n",
      "|   800/  900 batches | ms/batch 12.43 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 125 | time: 12.81s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.22 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 11.66 | loss  0.76 | perplexity     2.13\n",
      "|   600/  900 batches | ms/batch 11.67 | loss  0.75 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 11.65 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 126 | time: 12.60s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.73 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 11.67 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.09\n",
      "|   800/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 127 | time: 12.52s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.45 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.51 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.53 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  0.76 | perplexity     2.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 128 | time: 13.27s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.83 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.29 | loss  0.75 | perplexity     2.11\n",
      "|   600/  900 batches | ms/batch 12.58 | loss  0.74 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  0.75 | perplexity     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 129 | time: 13.35s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.62 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 12.36 | loss  0.75 | perplexity     2.11\n",
      "|   600/  900 batches | ms/batch 11.79 | loss  0.75 | perplexity     2.13\n",
      "|   800/  900 batches | ms/batch 12.51 | loss  0.75 | perplexity     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 130 | time: 13.21s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.90 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 12.79 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.38 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.07 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 131 | time: 13.31s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.13 | loss  0.75 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 12.10 | loss  0.75 | perplexity     2.11\n",
      "|   600/  900 batches | ms/batch 12.07 | loss  0.76 | perplexity     2.14\n",
      "|   800/  900 batches | ms/batch 12.08 | loss  0.76 | perplexity     2.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 132 | time: 12.96s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.15 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 12.55 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.81 | loss  0.74 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.85 | loss  0.74 | perplexity     2.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 133 | time: 13.47s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.71 | loss  0.75 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 12.52 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.51 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.84 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 134 | time: 13.49s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.71 | loss  0.75 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.57 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.65 | loss  0.74 | perplexity     2.10\n",
      "|   800/  900 batches | ms/batch 12.73 | loss  0.75 | perplexity     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 135 | time: 13.51s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.67 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 12.61 | loss  0.74 | perplexity     2.09\n",
      "|   600/  900 batches | ms/batch 12.56 | loss  0.74 | perplexity     2.09\n",
      "|   800/  900 batches | ms/batch 12.13 | loss  0.73 | perplexity     2.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 136 | time: 13.18s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.58 | loss  0.75 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 11.98 | loss  0.74 | perplexity     2.09\n",
      "|   600/  900 batches | ms/batch 11.70 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 11.78 | loss  0.75 | perplexity     2.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 137 | time: 12.78s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.72 | loss  0.74 | perplexity     2.10\n",
      "|   400/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.09\n",
      "|   600/  900 batches | ms/batch 11.67 | loss  0.74 | perplexity     2.10\n",
      "|   800/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 138 | time: 12.51s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.73 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.02 | loss  0.73 | perplexity     2.08\n",
      "|   800/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 139 | time: 12.58s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.75 | loss  0.75 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 11.74 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.09\n",
      "|   800/  900 batches | ms/batch 11.67 | loss  0.73 | perplexity     2.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 140 | time: 12.53s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.73 | loss  0.75 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 11.66 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.13 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.53 | loss  0.74 | perplexity     2.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 141 | time: 12.96s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.63 | loss  0.74 | perplexity     2.10\n",
      "|   400/  900 batches | ms/batch 12.48 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.44 | loss  0.74 | perplexity     2.10\n",
      "|   800/  900 batches | ms/batch 12.46 | loss  0.75 | perplexity     2.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 142 | time: 13.35s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.33 | loss  0.74 | perplexity     2.10\n",
      "|   400/  900 batches | ms/batch 12.07 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.06 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 143 | time: 12.98s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.15 | loss  0.76 | perplexity     2.13\n",
      "|   400/  900 batches | ms/batch 12.06 | loss  0.75 | perplexity     2.12\n",
      "|   600/  900 batches | ms/batch 12.06 | loss  0.74 | perplexity     2.10\n",
      "|   800/  900 batches | ms/batch 12.82 | loss  0.74 | perplexity     2.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 144 | time: 13.18s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.87 | loss  0.74 | perplexity     2.10\n",
      "|   400/  900 batches | ms/batch 12.75 | loss  0.74 | perplexity     2.09\n",
      "|   600/  900 batches | ms/batch 12.13 | loss  0.74 | perplexity     2.09\n",
      "|   800/  900 batches | ms/batch 12.06 | loss  0.76 | perplexity     2.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 145 | time: 13.24s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.12 | loss  0.75 | perplexity     2.12\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  0.74 | perplexity     2.09\n",
      "|   600/  900 batches | ms/batch 12.95 | loss  0.75 | perplexity     2.11\n",
      "|   800/  900 batches | ms/batch 12.74 | loss  0.74 | perplexity     2.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 146 | time: 13.36s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.96 | loss  0.74 | perplexity     2.11\n",
      "|   400/  900 batches | ms/batch 12.31 | loss  0.74 | perplexity     2.10\n",
      "|   600/  900 batches | ms/batch 12.24 | loss  0.74 | perplexity     2.09\n",
      "|   800/  900 batches | ms/batch 12.25 | loss  0.73 | perplexity     2.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 147 | time: 13.29s | test accuracy  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.31 | loss  0.72 | perplexity     2.06\n",
      "|   400/  900 batches | ms/batch 12.36 | loss  0.71 | perplexity     2.04\n",
      "|   600/  900 batches | ms/batch 12.91 | loss  0.72 | perplexity     2.05\n",
      "|   800/  900 batches | ms/batch 12.93 | loss  0.71 | perplexity     2.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 148 | time: 13.47s | test accuracy  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 13.01 | loss  0.71 | perplexity     2.02\n",
      "|   400/  900 batches | ms/batch 12.93 | loss  0.69 | perplexity     2.00\n",
      "|   600/  900 batches | ms/batch 12.93 | loss  0.70 | perplexity     2.01\n",
      "|   800/  900 batches | ms/batch 12.98 | loss  0.68 | perplexity     1.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 149 | time: 13.74s | test accuracy  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.55 | loss  0.68 | perplexity     1.98\n",
      "|   400/  900 batches | ms/batch 12.24 | loss  0.67 | perplexity     1.96\n",
      "|   600/  900 batches | ms/batch 12.56 | loss  0.67 | perplexity     1.96\n",
      "|   800/  900 batches | ms/batch 12.67 | loss  0.66 | perplexity     1.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time: 13.34s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.31 | loss  0.66 | perplexity     1.93\n",
      "|   400/  900 batches | ms/batch 12.26 | loss  0.65 | perplexity     1.91\n",
      "|   600/  900 batches | ms/batch 12.25 | loss  0.66 | perplexity     1.93\n",
      "|   800/  900 batches | ms/batch 12.25 | loss  0.65 | perplexity     1.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 151 | time: 13.15s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.34 | loss  0.64 | perplexity     1.90\n",
      "|   400/  900 batches | ms/batch 12.26 | loss  0.65 | perplexity     1.91\n",
      "|   600/  900 batches | ms/batch 12.24 | loss  0.64 | perplexity     1.90\n",
      "|   800/  900 batches | ms/batch 12.24 | loss  0.64 | perplexity     1.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 152 | time: 13.16s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.32 | loss  0.63 | perplexity     1.88\n",
      "|   400/  900 batches | ms/batch 12.68 | loss  0.63 | perplexity     1.87\n",
      "|   600/  900 batches | ms/batch 12.75 | loss  0.62 | perplexity     1.86\n",
      "|   800/  900 batches | ms/batch 12.56 | loss  0.63 | perplexity     1.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 153 | time: 13.40s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.36 | loss  0.62 | perplexity     1.86\n",
      "|   400/  900 batches | ms/batch 12.25 | loss  0.61 | perplexity     1.84\n",
      "|   600/  900 batches | ms/batch 12.24 | loss  0.62 | perplexity     1.85\n",
      "|   800/  900 batches | ms/batch 12.25 | loss  0.61 | perplexity     1.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 154 | time: 13.16s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.35 | loss  0.61 | perplexity     1.84\n",
      "|   400/  900 batches | ms/batch 12.79 | loss  0.61 | perplexity     1.83\n",
      "|   600/  900 batches | ms/batch 12.75 | loss  0.60 | perplexity     1.82\n",
      "|   800/  900 batches | ms/batch 12.87 | loss  0.61 | perplexity     1.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 155 | time: 13.49s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.32 | loss  0.60 | perplexity     1.82\n",
      "|   400/  900 batches | ms/batch 12.25 | loss  0.60 | perplexity     1.82\n",
      "|   600/  900 batches | ms/batch 12.25 | loss  0.60 | perplexity     1.83\n",
      "|   800/  900 batches | ms/batch 12.68 | loss  0.60 | perplexity     1.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 156 | time: 13.25s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.77 | loss  0.61 | perplexity     1.84\n",
      "|   400/  900 batches | ms/batch 12.83 | loss  0.59 | perplexity     1.81\n",
      "|   600/  900 batches | ms/batch 12.89 | loss  0.59 | perplexity     1.80\n",
      "|   800/  900 batches | ms/batch 12.90 | loss  0.58 | perplexity     1.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 157 | time: 13.68s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.94 | loss  0.59 | perplexity     1.81\n",
      "|   400/  900 batches | ms/batch 12.74 | loss  0.59 | perplexity     1.80\n",
      "|   600/  900 batches | ms/batch 12.83 | loss  0.58 | perplexity     1.78\n",
      "|   800/  900 batches | ms/batch 12.71 | loss  0.58 | perplexity     1.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 158 | time: 13.52s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.08 | loss  0.59 | perplexity     1.80\n",
      "|   400/  900 batches | ms/batch 12.00 | loss  0.57 | perplexity     1.78\n",
      "|   600/  900 batches | ms/batch 12.00 | loss  0.58 | perplexity     1.79\n",
      "|   800/  900 batches | ms/batch 11.99 | loss  0.57 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 159 | time: 12.88s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.07 | loss  0.57 | perplexity     1.78\n",
      "|   400/  900 batches | ms/batch 12.00 | loss  0.59 | perplexity     1.80\n",
      "|   600/  900 batches | ms/batch 12.01 | loss  0.57 | perplexity     1.78\n",
      "|   800/  900 batches | ms/batch 12.02 | loss  0.57 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 160 | time: 12.89s | test accuracy  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.56 | loss  0.58 | perplexity     1.78\n",
      "|   400/  900 batches | ms/batch 12.60 | loss  0.57 | perplexity     1.77\n",
      "|   600/  900 batches | ms/batch 12.40 | loss  0.57 | perplexity     1.77\n",
      "|   800/  900 batches | ms/batch 12.46 | loss  0.57 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 161 | time: 13.38s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.66 | loss  0.57 | perplexity     1.77\n",
      "|   400/  900 batches | ms/batch 12.53 | loss  0.57 | perplexity     1.76\n",
      "|   600/  900 batches | ms/batch 12.49 | loss  0.57 | perplexity     1.76\n",
      "|   800/  900 batches | ms/batch 12.53 | loss  0.57 | perplexity     1.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 162 | time: 13.37s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.27 | loss  0.56 | perplexity     1.75\n",
      "|   400/  900 batches | ms/batch 12.00 | loss  0.56 | perplexity     1.74\n",
      "|   600/  900 batches | ms/batch 11.99 | loss  0.55 | perplexity     1.73\n",
      "|   800/  900 batches | ms/batch 12.00 | loss  0.55 | perplexity     1.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 163 | time: 12.91s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.02 | loss  0.56 | perplexity     1.75\n",
      "|   400/  900 batches | ms/batch 11.65 | loss  0.56 | perplexity     1.75\n",
      "|   600/  900 batches | ms/batch 11.77 | loss  0.55 | perplexity     1.74\n",
      "|   800/  900 batches | ms/batch 12.36 | loss  0.56 | perplexity     1.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 164 | time: 12.83s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.08 | loss  0.55 | perplexity     1.74\n",
      "|   400/  900 batches | ms/batch 12.15 | loss  0.56 | perplexity     1.76\n",
      "|   600/  900 batches | ms/batch 12.30 | loss  0.56 | perplexity     1.74\n",
      "|   800/  900 batches | ms/batch 12.13 | loss  0.56 | perplexity     1.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 165 | time: 12.91s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.60 | loss  0.54 | perplexity     1.72\n",
      "|   400/  900 batches | ms/batch 11.55 | loss  0.55 | perplexity     1.73\n",
      "|   600/  900 batches | ms/batch 11.53 | loss  0.55 | perplexity     1.73\n",
      "|   800/  900 batches | ms/batch 11.53 | loss  0.55 | perplexity     1.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 166 | time: 12.38s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.61 | loss  0.55 | perplexity     1.74\n",
      "|   400/  900 batches | ms/batch 11.54 | loss  0.54 | perplexity     1.72\n",
      "|   600/  900 batches | ms/batch 11.68 | loss  0.54 | perplexity     1.72\n",
      "|   800/  900 batches | ms/batch 12.36 | loss  0.54 | perplexity     1.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 167 | time: 12.69s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.26 | loss  0.53 | perplexity     1.71\n",
      "|   400/  900 batches | ms/batch 12.39 | loss  0.55 | perplexity     1.72\n",
      "|   600/  900 batches | ms/batch 12.35 | loss  0.54 | perplexity     1.72\n",
      "|   800/  900 batches | ms/batch 12.32 | loss  0.55 | perplexity     1.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 168 | time: 13.13s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.47 | loss  0.54 | perplexity     1.72\n",
      "|   400/  900 batches | ms/batch 12.47 | loss  0.54 | perplexity     1.72\n",
      "|   600/  900 batches | ms/batch 12.34 | loss  0.53 | perplexity     1.71\n",
      "|   800/  900 batches | ms/batch 12.29 | loss  0.53 | perplexity     1.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 169 | time: 13.17s | test accuracy  0.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.95 | loss  0.54 | perplexity     1.72\n",
      "|   400/  900 batches | ms/batch 11.77 | loss  0.54 | perplexity     1.71\n",
      "|   600/  900 batches | ms/batch 11.77 | loss  0.54 | perplexity     1.71\n",
      "|   800/  900 batches | ms/batch 11.78 | loss  0.53 | perplexity     1.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 170 | time: 12.65s | test accuracy  0.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.84 | loss  0.54 | perplexity     1.71\n",
      "|   400/  900 batches | ms/batch 11.78 | loss  0.52 | perplexity     1.68\n",
      "|   600/  900 batches | ms/batch 11.78 | loss  0.53 | perplexity     1.70\n",
      "|   800/  900 batches | ms/batch 11.78 | loss  0.53 | perplexity     1.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 171 | time: 12.63s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 11.84 | loss  0.53 | perplexity     1.70\n",
      "|   400/  900 batches | ms/batch 11.77 | loss  0.52 | perplexity     1.68\n",
      "|   600/  900 batches | ms/batch 11.79 | loss  0.52 | perplexity     1.68\n",
      "|   800/  900 batches | ms/batch 12.55 | loss  0.53 | perplexity     1.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 172 | time: 12.85s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.78 | loss  0.53 | perplexity     1.69\n",
      "|   400/  900 batches | ms/batch 12.70 | loss  0.53 | perplexity     1.70\n",
      "|   600/  900 batches | ms/batch 12.31 | loss  0.52 | perplexity     1.68\n",
      "|   800/  900 batches | ms/batch 12.06 | loss  0.52 | perplexity     1.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 173 | time: 13.25s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.13 | loss  0.52 | perplexity     1.69\n",
      "|   400/  900 batches | ms/batch 12.07 | loss  0.53 | perplexity     1.71\n",
      "|   600/  900 batches | ms/batch 12.05 | loss  0.52 | perplexity     1.69\n",
      "|   800/  900 batches | ms/batch 12.06 | loss  0.53 | perplexity     1.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 174 | time: 12.95s | test accuracy  0.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.13 | loss  0.53 | perplexity     1.69\n",
      "|   400/  900 batches | ms/batch 12.05 | loss  0.52 | perplexity     1.69\n",
      "|   600/  900 batches | ms/batch 12.06 | loss  0.52 | perplexity     1.68\n",
      "|   800/  900 batches | ms/batch 12.10 | loss  0.51 | perplexity     1.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 175 | time: 12.96s | test accuracy  0.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/  900 batches | ms/batch 12.14 | loss  0.52 | perplexity     1.68\n",
      "|   400/  900 batches | ms/batch 12.65 | loss  0.52 | perplexity     1.68\n",
      "|   600/  900 batches | ms/batch 12.59 | loss  0.54 | perplexity     1.71\n",
      "|   800/  900 batches | ms/batch 12.67 | loss  0.51 | perplexity     1.67\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# get training results\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnbits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m results[nbits] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     38\u001b[0m train_epoch()\n\u001b[0;32m---> 39\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m accuracy_v_epoch\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m89\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m prompts \u001b[38;5;241m=\u001b[39m prompts\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (length_prompts, batch_size)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m target_answers \u001b[38;5;241m=\u001b[39m target_answers\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (length_answers + 1, batch_size)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_answers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (length_prompts + length_answers + 1, batch_size)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m answers_tokens \u001b[38;5;241m=\u001b[39m output[length_prompts:, :] \u001b[38;5;66;03m# (length_answers + 1, batch_size), contains tokens\u001b[39;00m\n\u001b[1;32m     12\u001b[0m equality_test \u001b[38;5;241m=\u001b[39m answers_tokens \u001b[38;5;241m==\u001b[39m target_answers \u001b[38;5;66;03m# (length_answers + 1, batch_size), contains boolean values\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(model, prompts, new_tokens)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(new_tokens):\n\u001b[0;32m----> 5\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (length_prompts, batch_size, ntokens)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     last_output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:] \u001b[38;5;66;03m# (batch_size, ntokens)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(last_output, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# (1, batch_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     27\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_emb(src) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mninp)\n\u001b[1;32m     28\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src)\n\u001b[0;32m---> 29\u001b[0m output_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m output_dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output_enc)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(output_dec, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), output_enc\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/transformer.py:391\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    388\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 391\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    394\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/transformer.py:714\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    712\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 714\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/transformer.py:722\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    721\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 722\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/functional.py:5380\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5375\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m bias_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5377\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   5378\u001b[0m \u001b[38;5;66;03m# reshape q, k, v for multihead attention and make em batch first\u001b[39;00m\n\u001b[1;32m   5379\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m-> 5380\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   5381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5382\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bsz \u001b[38;5;241m*\u001b[39m num_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "list_nbits = [3,4,5,6,7,8,9]\n",
    "list_epochs = [50,100,200,200,200,300,300]\n",
    "results = {}\n",
    "\n",
    "for epochs, nbits in zip(list_epochs,list_nbits):\n",
    "    # create dataset\n",
    "    data = []\n",
    "    for _ in range(dataset_size):\n",
    "        data.append(sample_datapoint(number_bits=nbits))\n",
    "    data_train = data[: int(train_proportion * dataset_size)]\n",
    "    data_test = data[int(train_proportion * dataset_size):]\n",
    "    # instantiate model\n",
    "    model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "    model.to(device)\n",
    "    # get training results\n",
    "    print(f\"Training {epochs} epochs on {nbits} bits\")\n",
    "    results[nbits] = train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83f49838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "for r, accs in results.items():\n",
    "    ax.plot(accs, label=f\"{r}-bits\")\n",
    "    \n",
    "ax.set_title(f\"test accuracy vs epoch\")\n",
    "ax.set_xlabel(f\"epochs\")\n",
    "ax.set_ylabel(f\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJ9IOZu8Xo4Y",
   "metadata": {
    "id": "qJ9IOZu8Xo4Y"
   },
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1213",
   "metadata": {},
   "source": [
    "This is just for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "yomPfirhXkLb",
   "metadata": {
    "id": "yomPfirhXkLb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def data_probing(size):\n",
    "    X = []\n",
    "    y = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        input = torch.tensor(tokenizer.encode(data[i][0])).view((-1, 1)).to(device)\n",
    "        _, output = model(input)\n",
    "        output = output[-1,:,:].flatten()\n",
    "        # determine whether there was a carry in the result:\n",
    "        carry = len(data[i][1]) > len(data[i][0]) / 2\n",
    "        X.append(output.cpu().detach().numpy())\n",
    "        y[i] = carry\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "QGmfXVxkppfP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGmfXVxkppfP",
    "outputId": "6601c884-004f-40bb-8a1a-71995b17d860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = data_probing(train_size)\n",
    "X_test, y_test = data_probing(test_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
