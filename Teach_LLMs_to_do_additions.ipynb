{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80517dbc",
   "metadata": {
    "id": "80517dbc"
   },
   "source": [
    "# Teach an LLM to do additions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaca18f",
   "metadata": {},
   "source": [
    "The goal of this project is to teach an LLM to do additions, playing only with two parts:\n",
    "* the tokenizer\n",
    "* the positional embedding\n",
    "\n",
    "Both the model and the dataset are fixed.\n",
    "\n",
    "You are allowed to tune the hyperparameters, but this is not the main goal. Depending on the quality of your tokenizer and positional embedding, you may change the number of bits. The initial value of 3 is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae993bb9",
   "metadata": {
    "id": "ae993bb9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OzGh9ahKF17h",
   "metadata": {
    "id": "OzGh9ahKF17h"
   },
   "outputs": [],
   "source": [
    "number_bits = 4\n",
    "\n",
    "dataset_size = 512_000\n",
    "train_proportion = 0.9\n",
    "\n",
    "log_interval = 200\n",
    "# large batch_size for GPU\n",
    "batch_size = 256\n",
    "# learning_rate beefed up\n",
    "learning_rate = 1e-3\n",
    "\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c054bed",
   "metadata": {
    "id": "6c054bed"
   },
   "source": [
    "## Step 1: Construct a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "t6aC9uNeIR6C",
   "metadata": {
    "id": "t6aC9uNeIR6C"
   },
   "outputs": [],
   "source": [
    "pad_token=\"[PAD]\"\n",
    "eos_token=\"[EOS]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BMvT0B-MGBnY",
   "metadata": {
    "id": "BMvT0B-MGBnY"
   },
   "source": [
    "### Baseline: character-level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "g2QiF-otFur3",
   "metadata": {
    "id": "g2QiF-otFur3"
   },
   "outputs": [],
   "source": [
    "class character_level_tokenizer:\n",
    "    \"\"\"\n",
    "    character-level\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = [str(x) for x in range(10)] + [\"+\", \"=\"] + [pad_token, eos_token]\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.vocab))}]\"\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the vocabulary\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "\n",
    "    def pre_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        character-level\n",
    "        \"\"\"\n",
    "        return [c for c in text]\n",
    "\n",
    "    def encode(self, text):\n",
    "        text_list = self.pre_tokenization(self.clean(text))\n",
    "        return [self.token_to_id[c] for c in text_list]\n",
    "\n",
    "    def decode(self, token_list):\n",
    "        return \"\".join([self.id_to_token[x] for x in token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "QuCc6jF5F8hK",
   "metadata": {
    "id": "QuCc6jF5F8hK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = character_level_tokenizer()\n",
    "ntokens = tokenizer.ntokens\n",
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8FXW2K-1Jd-P",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FXW2K-1Jd-P",
    "outputId": "349a4033-9fce-462b-f0d5-1bb3a7ffd340"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 10, 4, 2, 11], '12+42=')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"12 + 42 =\"\n",
    "inputs = tokenizer.encode(prompt)\n",
    "inputs, tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3gckvebGGYt",
   "metadata": {
    "id": "j3gckvebGGYt"
   },
   "source": [
    "# Implement your tokenizer here!\n",
    "\n",
    "You can do anything (as long as you do not compute the addition!).\n",
    "Some ideas:\n",
    "* reversing numbers left to right\n",
    "* arranging by groups (of, 2, 3,...)\n",
    "* aligning numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec36c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TheTentativeTokenizer:\n",
    "    \"\"\"OK, this is my attempt to make a tokenizer.\n",
    "    The idea is simple :\n",
    "    1- clean up : remove all characters that are not numeric, nor '+' nor '='\n",
    "    2- assuming the prompts are \"a + b =\" : \n",
    "        2.1- split the prompt to get two lists of characters corresponding to a and b\n",
    "        2.2- work backward, ie from the rightmost character to the leftmost character in a and b\n",
    "        2.3- if one of the character is None (end of list), replace by [PAD]\n",
    "        2.4- form tuples (character 1, character 2). This is a token.\n",
    "    3- return all tokens\n",
    "    As a result, the vocabulary is the set of tuples (char 1 x char 2) where char 1 and 2 are in [0-9, +, =, [PAD]]. Plus [EOS] special token.\n",
    "    \"\"\"\n",
    "    \n",
    "    pad_token=\"[PAD]\"\n",
    "    eos_token=\"[EOS]\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unit_token_list = [str(x) for x in range(10)] + [\"+\", \"=\"] + [self.pad_token, self.eos_token]\n",
    "        self.pattern = f\"[^{re.escape(''.join(self.unit_token_list))}]\"\n",
    "        self.vocab = [ (c1, c2) for c1 in self.unit_token_list for c2 in self.unit_token_list ] # not all tokens will ever be used, but never mind\n",
    "        self.token_to_id = {v : k for k, v in enumerate(self.vocab)}\n",
    "        self.id_to_token = {k : v for k, v in enumerate(self.vocab)}\n",
    "        self.ntokens = len(self.vocab)\n",
    "        \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        removes all characters not in the unit character list\n",
    "        \"\"\"\n",
    "        out = re.sub(self.pattern, \"\", text)\n",
    "        return out\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            text (_type_): prompt as an input. Assumes to be \"a + b = \" where a and b are strings with integers.\n",
    "\n",
    "        Returns:\n",
    "            tokens (list): list of tokens, each token being an integer\n",
    "        \"\"\"\n",
    "        \n",
    "        # remove all characters not in the vocabulary\n",
    "        cleaned_text = self.clean(text)\n",
    "        # split the prompt in two operands, remove everything else\n",
    "        pattern = r'\\+'\n",
    "        operands = re.split(pattern, cleaned_text)\n",
    "        # streamline the two operands\n",
    "        pattern = r'[ =]'\n",
    "        cleaned_operands = []\n",
    "        for op in operands:\n",
    "            c_op = re.sub(pattern, '', op)\n",
    "            cleaned_operands.append(c_op)\n",
    "        # build two lists of characters out of the cleaned operands\n",
    "        list1 = [c for c in cleaned_operands[0]]\n",
    "        list2 = [c for c in cleaned_operands[1]]\n",
    "        # work backward, egalize lengths and form tokens\n",
    "        tokens = []\n",
    "        pairs = []\n",
    "        list1.reverse()\n",
    "        list2.reverse()\n",
    "        if len(list1) < len(list2):\n",
    "            list1 = list1 + [self.pad_token] * (len(list2) - len(list1))\n",
    "        if len(list2) < len(list1):\n",
    "            list2 = list2 + [self.pad_token] * (len(list1) - len(list2))\n",
    "        # print(f\"list1 = {list1}\")\n",
    "        # print(f\"list2 = {list2}\")\n",
    "        for c1, c2 in zip(list1, list2):\n",
    "            pair = (c1, c2)\n",
    "            # print(pair)\n",
    "            tokens.append(self.token_to_id[pair])\n",
    "            pairs.append(pair)\n",
    "        \n",
    "        return tokens, pairs\n",
    "        \n",
    "    def decode(self, token_list):\n",
    "        \"\"\"Take the token list, find the associated pairs of characters, reverse and concatenate them to form the prompt\n",
    "\n",
    "        Args:\n",
    "            token_list (_type_): list of integers\n",
    "\n",
    "        Returns:\n",
    "            string : reconstructed prompt\n",
    "        \"\"\"\n",
    "        \n",
    "        list1 = []\n",
    "        list2 = []\n",
    "        # decompose the token list, find the associated pairs, create the two lists\n",
    "        for token in token_list:\n",
    "            pair = self.id_to_token[token]\n",
    "            c1 = pair[0]\n",
    "            c2 = pair[1]\n",
    "            if c1 != self.pad_token:\n",
    "                list1.append(c1)\n",
    "            if c2 != self.pad_token:\n",
    "                list2.append(c2)\n",
    "        # reverse and form the prompt\n",
    "        list1.reverse()\n",
    "        list2.reverse()\n",
    "        prompt = \"\".join(list1) + \" + \" + \"\".join(list2) + \" = \"\n",
    "            \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be232e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original prompt : '1245 + 347582 ='\n",
      "encoded prompt : [72, 64, 33, 21, 172, 171]\n",
      "pairs = [('5', '2'), ('4', '8'), ('2', '5'), ('1', '7'), ('[PAD]', '4'), ('[PAD]', '3')]\n",
      "decoded token list : '1245 + 347582 = '\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "\n",
    "tok = TheTentativeTokenizer()\n",
    "\n",
    "prompt = \"1245 + 347582 =\"\n",
    "print(f\"original prompt : '{prompt}'\")\n",
    "\n",
    "token_list, pair_list = tok.encode(prompt)\n",
    "print(f\"encoded prompt : {token_list}\")\n",
    "print(f\"pairs = {pair_list}\")\n",
    "\n",
    "dec = tok.decode(token_list=token_list)\n",
    "print(f\"decoded token list : '{dec}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491af297",
   "metadata": {
    "id": "491af297"
   },
   "source": [
    "## Step 2: Create a dataset for arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa90f31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daa90f31",
    "outputId": "3e8719ee-d8fa-4984-8b51-4db3457f7dbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4091+4859=', '8950')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_datapoint(number_bits = 3):\n",
    "    \"\"\"\n",
    "    returns a string containing two random numbers on `number_bits` many bits and their sum.\n",
    "    \"\"\"\n",
    "    a_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    b_list = [random.randint(0, 9) for _ in range(number_bits)]\n",
    "    a_int = int(\"\".join([str(x) for x in a_list]))\n",
    "    b_int = int(\"\".join([str(x) for x in b_list]))\n",
    "    sum_int = a_int + b_int\n",
    "    return (str(a_int) + \"+\" + str(b_int) + \"=\", str(sum_int))\n",
    "\n",
    "sample_datapoint(number_bits=number_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e861d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6e861d2",
    "outputId": "c88c2226-0546-473c-c296-88a52823886b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7051+9214=', '16265'),\n",
       " ('1533+8021=', '9554'),\n",
       " ('9683+8290=', '17973'),\n",
       " ('9354+2431=', '11785')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits))\n",
    "data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee85050",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fee85050",
    "outputId": "f080f4b0-fd76-48d8-d59f-7c118b6e6fe9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460800, 51200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "len(data_train),len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37200598",
   "metadata": {
    "id": "37200598"
   },
   "source": [
    "## Step 3: Construct a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7d2eb",
   "metadata": {},
   "source": [
    "### Baseline: the classical Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91674239",
   "metadata": {
    "id": "91674239"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
    "        Here, we use sine and cosine functions of different frequencies.\n",
    "    .. math:\n",
    "        \\text{PosEmbedder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEmbedder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # print(f\"position : {position.size()}\")\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # print(f\"pe = {pe}\")\n",
    "        # print(f\"pe = {pe.size()}\")\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296ceb2",
   "metadata": {},
   "source": [
    "# Implement your positional embedding here!\n",
    "\n",
    "You can do anything. Some ideas:\n",
    "* RoPE\n",
    "* (randomised) FIRE\n",
    "* Abacus\n",
    "\n",
    "**!!! IMPORTANT !!!** This model of Transformers is \"input first\", meaning that an input is a tensor with shape\n",
    "(length_prompts, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d4fe0",
   "metadata": {},
   "source": [
    "### Abacus embedding seems straightforward to implement with good results : https://arxiv.org/html/2405.17399v1#S3.F2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59795a4e",
   "metadata": {},
   "source": [
    "### Abacus \n",
    "\n",
    "https://arxiv.org/html/2405.17399v1#S3.F2\n",
    "\n",
    "Quote :\n",
    "\n",
    "\"To address the limitations of transformers at representing positional information, we design a specially built positional embedding that encodes the location of each digit relative to the start of the current number. We call this Abacus Embeddings. We apply the same positional embedding to all digits of the same significance, providing an explicit signal that the model can use to align digits. \n",
    "\n",
    "We take inspiration from Randomized Embeddings (Ruoss et al., 2023) but instead of using random ascending indices to represent positions in a sample, we use consecutive ascending indices with a random starting position to allow for length generalization. Specifically, during training we give consecutive positional embeddings to each digit in a number, starting from a randomly chosen offset value from $U \\in [1,k]$, where k is a hyperparameter. Unless otherwise stated the default value for k in this study is 100. \n",
    "\n",
    "For example, if the input is 123, the positional encodings are $\\beta, \\beta+1, \\beta+2$, where $\\beta \\sim U \\in [1,100]$ which are then passed through a learned embedding matrix. The value sampled from $U \\in [1,k]$ is the same for all numbers in a batch, meaning all digits of the same significance obtain the same positional embedding. This training scheme allows the model to see a wide range of positional embeddings, even when training sequences are short. At test time, each positional embedding begins from one, i.e. $\\beta = 1$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd8b95",
   "metadata": {},
   "source": [
    "### Some thoughts on positional encoding\n",
    "\n",
    "In our case, the numbers of same significance (ie at the same position in the operands) are already grouped by the tokenizer.\n",
    "\n",
    "So the position of the token itself, should be representative of the relative positions of each digit.\n",
    "\n",
    "Should the vanilla positional embedding be enough ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "589793fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TheTentativePositionalEmbedding(nn.Module):\n",
    "#         def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "#                 super(PositionalEmbedding, self).__init__()\n",
    "#                 self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         # pe = torch.zeros(max_len, d_model)\n",
    "#         # position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         # print(f\"position : {position.size()}\")\n",
    "#         # div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         # pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         # pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         # pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "#         # print(f\"pe = {pe}\")\n",
    "#         # print(f\"pe = {pe.size()}\")\n",
    "#         # self.register_buffer('pe', pe)\n",
    "        \n",
    "#         def forward(self, x):\n",
    "#                 r\"\"\"Inputs of forward function\n",
    "#                 Args:\n",
    "#                 x: the sequence fed to the positional encoder model (required).\n",
    "#                 Shape:\n",
    "#                 x: [sequence length, batch size, embed dim]\n",
    "#                 output: [sequence length, batch size, embed dim]\n",
    "#                 \"\"\"\n",
    "#                 pass\n",
    "\n",
    "#         # x = x + self.pe[:x.size(0), :]\n",
    "#         # return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb278ab",
   "metadata": {
    "id": "4eb278ab"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp,\n",
    "                                               nhead=nhead,\n",
    "                                               dim_feedforward=nhid,\n",
    "                                               num_encoder_layers=nlayers)\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEmbedding(ninp, dropout)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "        self.src_mask = mask\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output_enc = self.encoder(src, mask=self.src_mask)\n",
    "        output_dec = self.decoder(output_enc)\n",
    "        return F.log_softmax(output_dec, dim=-1), output_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f9d1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30e093a",
   "metadata": {},
   "source": [
    "Please do not change these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d568cc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d568cc4",
    "outputId": "f7f78975-2bdf-4c36-de35-3e140636d476"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin.deporte/.conda/envs/LLM/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=14, bias=True)\n",
       "  (input_emb): Embedding(14, 128)\n",
       "  (pos_encoder): PositionalEmbedding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f2f06e0",
   "metadata": {
    "id": "8f2f06e0"
   },
   "outputs": [],
   "source": [
    "def generate(model, prompts, new_tokens = 5):\n",
    "    input_tensor = prompts # (length_prompts, batch_size)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    for _ in range(new_tokens):\n",
    "        output, _ = model(input_tensor) # (length_prompts, batch_size, ntokens)\n",
    "        last_output = output[-1,:,:] # (batch_size, ntokens)\n",
    "        token = torch.argmax(last_output, -1).view((1,-1)) # (1, batch_size)\n",
    "        input_tensor = torch.cat((input_tensor, token), 0)\n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d76d1b19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d76d1b19",
    "outputId": "a1df1dc9-2ecc-4de4-85b2-6bc5bd460439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2, 10,  3, 11,  1,  1,  1,  1,  1]], device='cuda:0'), '2+3=11111')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"2+3=\"\n",
    "prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "output = generate(model, prompt_tensor).view((1,-1))\n",
    "output, tokenizer.decode(output.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00954ddc",
   "metadata": {
    "id": "00954ddc"
   },
   "outputs": [],
   "source": [
    "def pad(token_list, type_list = \"prompts\"):\n",
    "    max_length = max([len(x) for x in token_list])\n",
    "    out = []\n",
    "    for x in token_list:\n",
    "        if type_list == \"prompts\":\n",
    "            out.append([tokenizer.token_to_id[pad_token]] * (max_length - len(x)) + x)\n",
    "        if type_list == \"answers\":\n",
    "            out.append(x + [tokenizer.token_to_id[eos_token]] + [tokenizer.token_to_id[pad_token]] * (max_length - len(x)))\n",
    "    return out, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c84beab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c84beab",
    "outputId": "fc1bea13-d6e1-4a55-b70d-36de00bcec9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[PAD][PAD]1+1=', '21+35='], ['2[EOS][PAD]', '56[EOS]'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [tokenizer.encode(\"1+1=\"), tokenizer.encode(\"21+35=\")]\n",
    "answers = [tokenizer.encode(\"2\"), tokenizer.encode(\"56\")]\n",
    "padded_prompts, _ = pad(prompts, \"prompts\")\n",
    "padded_answers, _ = pad(answers, \"answers\")\n",
    "padded_prompts, padded_answers\n",
    "[tokenizer.decode(p) for p in padded_prompts], [tokenizer.decode(p) for p in padded_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "264f9227",
   "metadata": {
    "id": "264f9227"
   },
   "outputs": [],
   "source": [
    "def get_batch(split, i):\n",
    "    data = data_train if split == 'train' else data_test\n",
    "    prompts = [tokenizer.encode(data[i][0]) for i in range(i, i + batch_size)]\n",
    "    padded_prompts, length_prompts = pad(prompts, \"prompts\")\n",
    "    answers = [tokenizer.encode(data[i][1]) for i in range(i, i + batch_size)]\n",
    "    padded_answers, length_answers = pad(answers, \"answers\")\n",
    "    X = torch.stack([torch.tensor(x) for x in padded_prompts], 1)\n",
    "    Y = torch.stack([torch.tensor(x) for x in padded_answers], 1)\n",
    "    return X, Y, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e281ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91e281ad",
    "outputId": "22e2d0ee-ede4-41f8-e089-fb63ac2d9787"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 256]), torch.Size([6, 256]), 10, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y, length_prompts, length_answers = get_batch(\"train\", 243)\n",
    "X.shape, Y.shape, length_prompts, length_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e1fd1",
   "metadata": {
    "id": "113e1fd1"
   },
   "source": [
    "## Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cfcd10a",
   "metadata": {
    "id": "1cfcd10a"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # Turn on evaluation mode disables dropout.\n",
    "    model.eval()\n",
    "    correct = 0.\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, len(data_test) - 1, batch_size)):\n",
    "            prompts, target_answers, length_prompts, length_answers = get_batch(\"test\", i)\n",
    "            prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "            target_answers = target_answers.to(device) # (length_answers + 1, batch_size)\n",
    "            output = generate(model, prompts, length_answers + 1) # (length_prompts + length_answers + 1, batch_size)\n",
    "            answers_tokens = output[length_prompts:, :] # (length_answers + 1, batch_size), contains tokens\n",
    "            equality_test = answers_tokens == target_answers # (length_answers + 1, batch_size), contains boolean values\n",
    "            correct += torch.all(equality_test, axis=0).float().sum()\n",
    "        accuracy = correct / len(data_test)\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac335b05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac335b05",
    "outputId": "b475e943-51b3-401d-d18b-c9d32a49ffb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54061a",
   "metadata": {
    "id": "4c54061a"
   },
   "source": [
    "## Step 4: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3638a75d",
   "metadata": {
    "id": "3638a75d"
   },
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(0, len(data_train) - 1, batch_size)):\n",
    "        prompts, target_answers, length_prompts, length_answers = get_batch(\"train\", i)\n",
    "        prompts = prompts.to(device) # (length_prompts, batch_size)\n",
    "        target_answers = target_answers.to(device) # (length_answers, batch_size)\n",
    "        input_tensor = torch.cat((prompts, target_answers), 0) # (length_prompts + length_answers, batch_size)\n",
    "        model.zero_grad()\n",
    "        output, _ = model(input_tensor) # (length_prompts + length_answers, batch_size, ntokens)\n",
    "        output_answers = output[length_prompts-1:-1,:,:].reshape(-1, ntokens) # (length_answers * batch_size, ntokens)\n",
    "        target_answers = target_answers.view(-1)\n",
    "        loss = F.cross_entropy(output_answers, target_answers)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.2f} | perplexity {:8.2f}'.format(batch, len(data_train) // batch_size,\n",
    "                                                                                                        elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def train(epochs=3):\n",
    "    accuracy_v_epoch = []\n",
    "    best_test_accuracy = None\n",
    "    test_accuracy = evaluate()\n",
    "    print('-' * 89)\n",
    "    print('| initialisation | test accuracy {:5.2f}'.format(test_accuracy))\n",
    "    print('-' * 89)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_epoch()\n",
    "        test_accuracy = evaluate()\n",
    "        accuracy_v_epoch.append(test_accuracy)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:5.2f}'.format(epoch, (time.time() - epoch_start_time), test_accuracy))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the test accuracy is the best we've seen so far.\n",
    "        if not best_test_accuracy or test_accuracy < best_test_accuracy:\n",
    "            with open(\"arithmetic.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_test_accuracy = test_accuracy\n",
    "            \n",
    "    return accuracy_v_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "143b3951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25592317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset à 3 bits : ('313+45=', '358')\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 19.96 | loss  1.70 | perplexity     5.48\n",
      "|   400/ 1800 batches | ms/batch 19.86 | loss  1.27 | perplexity     3.57\n",
      "|   600/ 1800 batches | ms/batch 19.86 | loss  1.16 | perplexity     3.18\n",
      "|   800/ 1800 batches | ms/batch 20.39 | loss  1.10 | perplexity     3.01\n",
      "|  1000/ 1800 batches | ms/batch 19.93 | loss  1.07 | perplexity     2.92\n",
      "|  1200/ 1800 batches | ms/batch 19.92 | loss  1.06 | perplexity     2.88\n",
      "|  1400/ 1800 batches | ms/batch 19.90 | loss  1.04 | perplexity     2.83\n",
      "|  1600/ 1800 batches | ms/batch 19.87 | loss  1.03 | perplexity     2.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 40.48s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 19.96 | loss  1.02 | perplexity     2.77\n",
      "|   400/ 1800 batches | ms/batch 20.30 | loss  1.01 | perplexity     2.73\n",
      "|   600/ 1800 batches | ms/batch 19.85 | loss  0.92 | perplexity     2.52\n",
      "|   800/ 1800 batches | ms/batch 19.85 | loss  0.82 | perplexity     2.27\n",
      "|  1000/ 1800 batches | ms/batch 19.85 | loss  0.75 | perplexity     2.11\n",
      "|  1200/ 1800 batches | ms/batch 19.85 | loss  0.70 | perplexity     2.02\n",
      "|  1400/ 1800 batches | ms/batch 19.85 | loss  0.67 | perplexity     1.95\n",
      "|  1600/ 1800 batches | ms/batch 19.87 | loss  0.64 | perplexity     1.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 40.53s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 19.95 | loss  0.60 | perplexity     1.83\n",
      "|   400/ 1800 batches | ms/batch 19.85 | loss  0.57 | perplexity     1.77\n",
      "|   600/ 1800 batches | ms/batch 19.85 | loss  0.48 | perplexity     1.62\n",
      "|   800/ 1800 batches | ms/batch 19.96 | loss  0.42 | perplexity     1.52\n",
      "|  1000/ 1800 batches | ms/batch 19.86 | loss  0.31 | perplexity     1.36\n",
      "|  1200/ 1800 batches | ms/batch 19.86 | loss  0.25 | perplexity     1.28\n",
      "|  1400/ 1800 batches | ms/batch 20.45 | loss  0.19 | perplexity     1.21\n",
      "|  1600/ 1800 batches | ms/batch 20.01 | loss  0.16 | perplexity     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 40.62s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.20 | loss  0.14 | perplexity     1.15\n",
      "|   400/ 1800 batches | ms/batch 20.08 | loss  0.12 | perplexity     1.13\n",
      "|   600/ 1800 batches | ms/batch 20.09 | loss  0.11 | perplexity     1.12\n",
      "|   800/ 1800 batches | ms/batch 20.07 | loss  0.11 | perplexity     1.11\n",
      "|  1000/ 1800 batches | ms/batch 20.56 | loss  0.09 | perplexity     1.10\n",
      "|  1200/ 1800 batches | ms/batch 20.07 | loss  0.09 | perplexity     1.09\n",
      "|  1400/ 1800 batches | ms/batch 20.06 | loss  0.08 | perplexity     1.08\n",
      "|  1600/ 1800 batches | ms/batch 19.97 | loss  0.08 | perplexity     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 40.91s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nbits = 3\n",
    "results = {}\n",
    "\n",
    "# create dataset\n",
    "data = []\n",
    "for _ in range(dataset_size):\n",
    "    data.append(sample_datapoint(number_bits=nbits))\n",
    "data_train = data[: int(train_proportion * dataset_size)]\n",
    "data_test = data[int(train_proportion * dataset_size):]\n",
    "\n",
    "# reporting out\n",
    "print(f\"Dataset à {nbits} bits : {data_train[0]}\")\n",
    "\n",
    "# instantiate model\n",
    "model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "model.to(device)\n",
    "\n",
    "# go\n",
    "learning_rate = 1e-3\n",
    "epochs = 4\n",
    "\n",
    "acc_v_epoch = train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56d9d440",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56d9d440",
    "outputId": "1872232b-b120-440b-e1a6-666e079efa3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483+458=941\t actual result: 941\n",
      "95+542=147\t actual result: 637\n",
      "372+264=636\t actual result: 636\n",
      "472+971=1443\t actual result: 1443\n",
      "943+387=1330\t actual result: 1330\n",
      "238+707=945\t actual result: 945\n",
      "772+33=111\t actual result: 805\n",
      "104+616=720\t actual result: 720\n",
      "596+833=1429\t actual result: 1429\n",
      "738+481=1219\t actual result: 1219\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "for i in range(10):\n",
    "    prompt, answers = data_test[i]\n",
    "    prompt_tensor = torch.tensor(tokenizer.encode(prompt)).view((-1,1))\n",
    "    output = generate(model, prompt_tensor, len(answers)).view((1,-1))\n",
    "    print(tokenizer.decode(output.tolist()[0]) + \"\\t actual result: \" + answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ec1eafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(acc_v_epoch, label={number_bits})\n",
    "ax.set_title(f\"test accuracy vs epoch - {nbits} bits\")\n",
    "ax.set_xlabel(f\"epochs\")\n",
    "ax.set_ylabel(f\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25594b26",
   "metadata": {},
   "source": [
    "# Step 5 : testing with different number of bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5590bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset à 3 bits : ('421+921=', '1342')\n",
      "Training 5 epochs on 3 bits\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.02 | loss  1.67 | perplexity     5.31\n",
      "|   400/ 1800 batches | ms/batch 20.39 | loss  1.28 | perplexity     3.59\n",
      "|   600/ 1800 batches | ms/batch 19.91 | loss  1.16 | perplexity     3.20\n",
      "|   800/ 1800 batches | ms/batch 19.91 | loss  1.10 | perplexity     3.01\n",
      "|  1000/ 1800 batches | ms/batch 19.95 | loss  1.07 | perplexity     2.93\n",
      "|  1200/ 1800 batches | ms/batch 20.00 | loss  1.05 | perplexity     2.87\n",
      "|  1400/ 1800 batches | ms/batch 20.02 | loss  1.04 | perplexity     2.83\n",
      "|  1600/ 1800 batches | ms/batch 20.02 | loss  1.03 | perplexity     2.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 40.79s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.22 | loss  1.01 | perplexity     2.76\n",
      "|   400/ 1800 batches | ms/batch 20.02 | loss  0.98 | perplexity     2.66\n",
      "|   600/ 1800 batches | ms/batch 20.03 | loss  0.88 | perplexity     2.42\n",
      "|   800/ 1800 batches | ms/batch 20.00 | loss  0.78 | perplexity     2.18\n",
      "|  1000/ 1800 batches | ms/batch 20.00 | loss  0.73 | perplexity     2.08\n",
      "|  1200/ 1800 batches | ms/batch 20.02 | loss  0.68 | perplexity     1.98\n",
      "|  1400/ 1800 batches | ms/batch 20.38 | loss  0.66 | perplexity     1.93\n",
      "|  1600/ 1800 batches | ms/batch 19.90 | loss  0.64 | perplexity     1.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 40.77s | test accuracy  0.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.13 | loss  0.61 | perplexity     1.84\n",
      "|   400/ 1800 batches | ms/batch 20.05 | loss  0.59 | perplexity     1.80\n",
      "|   600/ 1800 batches | ms/batch 20.03 | loss  0.59 | perplexity     1.80\n",
      "|   800/ 1800 batches | ms/batch 20.01 | loss  0.57 | perplexity     1.76\n",
      "|  1000/ 1800 batches | ms/batch 20.49 | loss  0.53 | perplexity     1.71\n",
      "|  1200/ 1800 batches | ms/batch 20.04 | loss  0.47 | perplexity     1.60\n",
      "|  1400/ 1800 batches | ms/batch 20.05 | loss  0.33 | perplexity     1.39\n",
      "|  1600/ 1800 batches | ms/batch 20.05 | loss  0.25 | perplexity     1.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 40.83s | test accuracy  0.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.15 | loss  0.16 | perplexity     1.18\n",
      "|   400/ 1800 batches | ms/batch 20.05 | loss  0.14 | perplexity     1.15\n",
      "|   600/ 1800 batches | ms/batch 20.49 | loss  0.13 | perplexity     1.14\n",
      "|   800/ 1800 batches | ms/batch 19.95 | loss  0.11 | perplexity     1.11\n",
      "|  1000/ 1800 batches | ms/batch 19.95 | loss  0.09 | perplexity     1.10\n",
      "|  1200/ 1800 batches | ms/batch 19.95 | loss  0.09 | perplexity     1.09\n",
      "|  1400/ 1800 batches | ms/batch 19.95 | loss  0.08 | perplexity     1.09\n",
      "|  1600/ 1800 batches | ms/batch 19.97 | loss  0.09 | perplexity     1.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 40.71s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.55 | loss  0.08 | perplexity     1.08\n",
      "|   400/ 1800 batches | ms/batch 19.99 | loss  0.07 | perplexity     1.08\n",
      "|   600/ 1800 batches | ms/batch 19.99 | loss  0.07 | perplexity     1.07\n",
      "|   800/ 1800 batches | ms/batch 19.99 | loss  0.07 | perplexity     1.08\n",
      "|  1000/ 1800 batches | ms/batch 19.96 | loss  0.07 | perplexity     1.08\n",
      "|  1200/ 1800 batches | ms/batch 19.97 | loss  0.07 | perplexity     1.07\n",
      "|  1400/ 1800 batches | ms/batch 19.97 | loss  0.06 | perplexity     1.06\n",
      "|  1600/ 1800 batches | ms/batch 19.96 | loss  0.06 | perplexity     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 40.78s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "Dataset à 4 bits : ('5715+2226=', '7941')\n",
      "Training 10 epochs on 4 bits\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.77 | loss  1.78 | perplexity     5.91\n",
      "|   400/ 1800 batches | ms/batch 20.70 | loss  1.43 | perplexity     4.19\n",
      "|   600/ 1800 batches | ms/batch 20.68 | loss  1.34 | perplexity     3.82\n",
      "|   800/ 1800 batches | ms/batch 20.72 | loss  1.30 | perplexity     3.66\n",
      "|  1000/ 1800 batches | ms/batch 20.70 | loss  1.28 | perplexity     3.59\n",
      "|  1200/ 1800 batches | ms/batch 21.16 | loss  1.26 | perplexity     3.53\n",
      "|  1400/ 1800 batches | ms/batch 20.67 | loss  1.25 | perplexity     3.48\n",
      "|  1600/ 1800 batches | ms/batch 20.67 | loss  1.24 | perplexity     3.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 43.12s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.84 | loss  1.24 | perplexity     3.47\n",
      "|   400/ 1800 batches | ms/batch 20.66 | loss  1.21 | perplexity     3.37\n",
      "|   600/ 1800 batches | ms/batch 20.66 | loss  1.20 | perplexity     3.31\n",
      "|   800/ 1800 batches | ms/batch 21.14 | loss  1.11 | perplexity     3.02\n",
      "|  1000/ 1800 batches | ms/batch 20.67 | loss  1.01 | perplexity     2.73\n",
      "|  1200/ 1800 batches | ms/batch 20.66 | loss  0.97 | perplexity     2.63\n",
      "|  1400/ 1800 batches | ms/batch 20.67 | loss  0.94 | perplexity     2.56\n",
      "|  1600/ 1800 batches | ms/batch 20.66 | loss  0.92 | perplexity     2.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 43.10s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 21.24 | loss  0.90 | perplexity     2.46\n",
      "|   400/ 1800 batches | ms/batch 20.73 | loss  0.89 | perplexity     2.43\n",
      "|   600/ 1800 batches | ms/batch 20.75 | loss  0.87 | perplexity     2.40\n",
      "|   800/ 1800 batches | ms/batch 20.76 | loss  0.87 | perplexity     2.39\n",
      "|  1000/ 1800 batches | ms/batch 20.75 | loss  0.87 | perplexity     2.38\n",
      "|  1200/ 1800 batches | ms/batch 20.72 | loss  0.87 | perplexity     2.38\n",
      "|  1400/ 1800 batches | ms/batch 20.74 | loss  0.85 | perplexity     2.35\n",
      "|  1600/ 1800 batches | ms/batch 20.74 | loss  0.86 | perplexity     2.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 43.34s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.86 | loss  0.86 | perplexity     2.36\n",
      "|   400/ 1800 batches | ms/batch 20.72 | loss  0.84 | perplexity     2.33\n",
      "|   600/ 1800 batches | ms/batch 20.76 | loss  0.85 | perplexity     2.33\n",
      "|   800/ 1800 batches | ms/batch 20.75 | loss  0.84 | perplexity     2.33\n",
      "|  1000/ 1800 batches | ms/batch 20.72 | loss  0.84 | perplexity     2.32\n",
      "|  1200/ 1800 batches | ms/batch 20.74 | loss  0.84 | perplexity     2.32\n",
      "|  1400/ 1800 batches | ms/batch 21.13 | loss  0.83 | perplexity     2.30\n",
      "|  1600/ 1800 batches | ms/batch 20.66 | loss  0.84 | perplexity     2.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 43.18s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.77 | loss  0.84 | perplexity     2.32\n",
      "|   400/ 1800 batches | ms/batch 20.67 | loss  0.83 | perplexity     2.30\n",
      "|   600/ 1800 batches | ms/batch 20.67 | loss  0.83 | perplexity     2.30\n",
      "|   800/ 1800 batches | ms/batch 20.73 | loss  0.83 | perplexity     2.30\n",
      "|  1000/ 1800 batches | ms/batch 21.21 | loss  0.83 | perplexity     2.30\n",
      "|  1200/ 1800 batches | ms/batch 20.71 | loss  0.83 | perplexity     2.29\n",
      "|  1400/ 1800 batches | ms/batch 20.71 | loss  0.83 | perplexity     2.29\n",
      "|  1600/ 1800 batches | ms/batch 20.71 | loss  0.83 | perplexity     2.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 43.15s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.88 | loss  0.81 | perplexity     2.24\n",
      "|   400/ 1800 batches | ms/batch 20.77 | loss  0.77 | perplexity     2.16\n",
      "|   600/ 1800 batches | ms/batch 21.18 | loss  0.73 | perplexity     2.08\n",
      "|   800/ 1800 batches | ms/batch 20.71 | loss  0.70 | perplexity     2.02\n",
      "|  1000/ 1800 batches | ms/batch 20.72 | loss  0.68 | perplexity     1.97\n",
      "|  1200/ 1800 batches | ms/batch 20.71 | loss  0.65 | perplexity     1.91\n",
      "|  1400/ 1800 batches | ms/batch 20.77 | loss  0.63 | perplexity     1.89\n",
      "|  1600/ 1800 batches | ms/batch 20.77 | loss  0.62 | perplexity     1.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 43.26s | test accuracy  0.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 21.29 | loss  0.59 | perplexity     1.81\n",
      "|   400/ 1800 batches | ms/batch 20.77 | loss  0.58 | perplexity     1.79\n",
      "|   600/ 1800 batches | ms/batch 20.78 | loss  0.57 | perplexity     1.77\n",
      "|   800/ 1800 batches | ms/batch 20.77 | loss  0.56 | perplexity     1.75\n",
      "|  1000/ 1800 batches | ms/batch 20.77 | loss  0.55 | perplexity     1.74\n",
      "|  1200/ 1800 batches | ms/batch 20.76 | loss  0.54 | perplexity     1.71\n",
      "|  1400/ 1800 batches | ms/batch 20.76 | loss  0.55 | perplexity     1.73\n",
      "|  1600/ 1800 batches | ms/batch 20.75 | loss  0.53 | perplexity     1.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 43.40s | test accuracy  0.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.86 | loss  0.52 | perplexity     1.67\n",
      "|   400/ 1800 batches | ms/batch 20.75 | loss  0.50 | perplexity     1.65\n",
      "|   600/ 1800 batches | ms/batch 20.75 | loss  0.48 | perplexity     1.61\n",
      "|   800/ 1800 batches | ms/batch 20.76 | loss  0.43 | perplexity     1.54\n",
      "|  1000/ 1800 batches | ms/batch 20.75 | loss  0.42 | perplexity     1.51\n",
      "|  1200/ 1800 batches | ms/batch 20.75 | loss  0.38 | perplexity     1.47\n",
      "|  1400/ 1800 batches | ms/batch 21.25 | loss  0.38 | perplexity     1.46\n",
      "|  1600/ 1800 batches | ms/batch 20.74 | loss  0.35 | perplexity     1.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 43.27s | test accuracy  0.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.86 | loss  0.26 | perplexity     1.30\n",
      "|   400/ 1800 batches | ms/batch 20.75 | loss  0.23 | perplexity     1.26\n",
      "|   600/ 1800 batches | ms/batch 20.75 | loss  0.22 | perplexity     1.25\n",
      "|   800/ 1800 batches | ms/batch 20.74 | loss  0.21 | perplexity     1.23\n",
      "|  1000/ 1800 batches | ms/batch 21.26 | loss  0.19 | perplexity     1.21\n",
      "|  1200/ 1800 batches | ms/batch 20.76 | loss  0.19 | perplexity     1.20\n",
      "|  1400/ 1800 batches | ms/batch 20.76 | loss  0.17 | perplexity     1.18\n",
      "|  1600/ 1800 batches | ms/batch 20.75 | loss  0.16 | perplexity     1.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 43.28s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 20.88 | loss  0.15 | perplexity     1.16\n",
      "|   400/ 1800 batches | ms/batch 21.27 | loss  0.14 | perplexity     1.15\n",
      "|   600/ 1800 batches | ms/batch 20.72 | loss  0.13 | perplexity     1.14\n",
      "|   800/ 1800 batches | ms/batch 20.71 | loss  0.13 | perplexity     1.14\n",
      "|  1000/ 1800 batches | ms/batch 20.72 | loss  0.13 | perplexity     1.14\n",
      "|  1200/ 1800 batches | ms/batch 20.72 | loss  0.12 | perplexity     1.13\n",
      "|  1400/ 1800 batches | ms/batch 20.71 | loss  0.11 | perplexity     1.11\n",
      "|  1600/ 1800 batches | ms/batch 20.72 | loss  0.10 | perplexity     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 43.34s | test accuracy  0.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "Dataset à 5 bits : ('78766+62229=', '140995')\n",
      "Training 10 epochs on 5 bits\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.25 | loss  1.92 | perplexity     6.83\n",
      "|   400/ 1800 batches | ms/batch 22.15 | loss  1.61 | perplexity     4.99\n",
      "|   600/ 1800 batches | ms/batch 22.16 | loss  1.50 | perplexity     4.47\n",
      "|   800/ 1800 batches | ms/batch 22.11 | loss  1.46 | perplexity     4.29\n",
      "|  1000/ 1800 batches | ms/batch 22.13 | loss  1.43 | perplexity     4.18\n",
      "|  1200/ 1800 batches | ms/batch 22.18 | loss  1.41 | perplexity     4.11\n",
      "|  1400/ 1800 batches | ms/batch 22.66 | loss  1.41 | perplexity     4.08\n",
      "|  1600/ 1800 batches | ms/batch 22.14 | loss  1.39 | perplexity     4.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 47.05s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.28 | loss  1.39 | perplexity     4.02\n",
      "|   400/ 1800 batches | ms/batch 22.14 | loss  1.37 | perplexity     3.94\n",
      "|   600/ 1800 batches | ms/batch 22.16 | loss  1.35 | perplexity     3.85\n",
      "|   800/ 1800 batches | ms/batch 22.15 | loss  1.24 | perplexity     3.46\n",
      "|  1000/ 1800 batches | ms/batch 22.64 | loss  1.19 | perplexity     3.29\n",
      "|  1200/ 1800 batches | ms/batch 22.16 | loss  1.16 | perplexity     3.20\n",
      "|  1400/ 1800 batches | ms/batch 22.17 | loss  1.15 | perplexity     3.14\n",
      "|  1600/ 1800 batches | ms/batch 22.16 | loss  1.13 | perplexity     3.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 47.09s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.29 | loss  1.13 | perplexity     3.09\n",
      "|   400/ 1800 batches | ms/batch 22.20 | loss  1.11 | perplexity     3.02\n",
      "|   600/ 1800 batches | ms/batch 22.65 | loss  1.10 | perplexity     3.01\n",
      "|   800/ 1800 batches | ms/batch 22.13 | loss  1.09 | perplexity     2.99\n",
      "|  1000/ 1800 batches | ms/batch 22.12 | loss  1.09 | perplexity     2.97\n",
      "|  1200/ 1800 batches | ms/batch 22.14 | loss  1.09 | perplexity     2.97\n",
      "|  1400/ 1800 batches | ms/batch 22.15 | loss  1.08 | perplexity     2.95\n",
      "|  1600/ 1800 batches | ms/batch 22.15 | loss  1.08 | perplexity     2.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 47.07s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.74 | loss  1.08 | perplexity     2.94\n",
      "|   400/ 1800 batches | ms/batch 22.17 | loss  1.07 | perplexity     2.92\n",
      "|   600/ 1800 batches | ms/batch 22.14 | loss  1.06 | perplexity     2.89\n",
      "|   800/ 1800 batches | ms/batch 22.17 | loss  1.07 | perplexity     2.91\n",
      "|  1000/ 1800 batches | ms/batch 22.14 | loss  1.06 | perplexity     2.89\n",
      "|  1200/ 1800 batches | ms/batch 22.12 | loss  1.06 | perplexity     2.88\n",
      "|  1400/ 1800 batches | ms/batch 22.13 | loss  1.05 | perplexity     2.87\n",
      "|  1600/ 1800 batches | ms/batch 22.13 | loss  1.06 | perplexity     2.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 47.14s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.20 | loss  1.06 | perplexity     2.88\n",
      "|   400/ 1800 batches | ms/batch 22.11 | loss  1.05 | perplexity     2.86\n",
      "|   600/ 1800 batches | ms/batch 22.13 | loss  1.05 | perplexity     2.85\n",
      "|   800/ 1800 batches | ms/batch 22.13 | loss  1.05 | perplexity     2.86\n",
      "|  1000/ 1800 batches | ms/batch 22.09 | loss  1.05 | perplexity     2.86\n",
      "|  1200/ 1800 batches | ms/batch 22.54 | loss  1.05 | perplexity     2.85\n",
      "|  1400/ 1800 batches | ms/batch 22.10 | loss  1.05 | perplexity     2.84\n",
      "|  1600/ 1800 batches | ms/batch 22.13 | loss  1.05 | perplexity     2.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 46.99s | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.24 | loss  1.05 | perplexity     2.85\n",
      "|   400/ 1800 batches | ms/batch 22.09 | loss  1.04 | perplexity     2.84\n",
      "|   600/ 1800 batches | ms/batch 22.08 | loss  1.05 | perplexity     2.85\n",
      "|   800/ 1800 batches | ms/batch 22.55 | loss  1.04 | perplexity     2.84\n",
      "|  1000/ 1800 batches | ms/batch 22.09 | loss  1.03 | perplexity     2.80\n",
      "|  1200/ 1800 batches | ms/batch 22.13 | loss  0.99 | perplexity     2.68\n",
      "|  1400/ 1800 batches | ms/batch 22.13 | loss  0.94 | perplexity     2.57\n",
      "|  1600/ 1800 batches | ms/batch 22.12 | loss  0.91 | perplexity     2.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 46.98s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.34 | loss  0.87 | perplexity     2.39\n",
      "|   400/ 1800 batches | ms/batch 22.68 | loss  0.86 | perplexity     2.35\n",
      "|   600/ 1800 batches | ms/batch 22.26 | loss  0.85 | perplexity     2.35\n",
      "|   800/ 1800 batches | ms/batch 22.26 | loss  0.84 | perplexity     2.33\n",
      "|  1000/ 1800 batches | ms/batch 22.21 | loss  0.84 | perplexity     2.31\n",
      "|  1200/ 1800 batches | ms/batch 22.21 | loss  0.84 | perplexity     2.31\n",
      "|  1400/ 1800 batches | ms/batch 22.22 | loss  0.83 | perplexity     2.30\n",
      "|  1600/ 1800 batches | ms/batch 22.18 | loss  0.82 | perplexity     2.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 47.29s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.29 | loss  0.82 | perplexity     2.27\n",
      "|   400/ 1800 batches | ms/batch 22.16 | loss  0.80 | perplexity     2.23\n",
      "|   600/ 1800 batches | ms/batch 22.16 | loss  0.80 | perplexity     2.23\n",
      "|   800/ 1800 batches | ms/batch 22.17 | loss  0.81 | perplexity     2.24\n",
      "|  1000/ 1800 batches | ms/batch 22.19 | loss  0.80 | perplexity     2.22\n",
      "|  1200/ 1800 batches | ms/batch 22.19 | loss  0.81 | perplexity     2.24\n",
      "|  1400/ 1800 batches | ms/batch 22.20 | loss  0.80 | perplexity     2.22\n",
      "|  1600/ 1800 batches | ms/batch 22.64 | loss  0.79 | perplexity     2.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 47.10s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.29 | loss  0.79 | perplexity     2.21\n",
      "|   400/ 1800 batches | ms/batch 22.17 | loss  0.78 | perplexity     2.19\n",
      "|   600/ 1800 batches | ms/batch 22.18 | loss  0.79 | perplexity     2.20\n",
      "|   800/ 1800 batches | ms/batch 22.17 | loss  0.79 | perplexity     2.20\n",
      "|  1000/ 1800 batches | ms/batch 22.18 | loss  0.79 | perplexity     2.21\n",
      "|  1200/ 1800 batches | ms/batch 22.67 | loss  0.78 | perplexity     2.18\n",
      "|  1400/ 1800 batches | ms/batch 22.20 | loss  0.78 | perplexity     2.17\n",
      "|  1600/ 1800 batches | ms/batch 22.19 | loss  0.78 | perplexity     2.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 47.14s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 22.32 | loss  0.78 | perplexity     2.18\n",
      "|   400/ 1800 batches | ms/batch 22.20 | loss  0.77 | perplexity     2.16\n",
      "|   600/ 1800 batches | ms/batch 22.20 | loss  0.78 | perplexity     2.19\n",
      "|   800/ 1800 batches | ms/batch 22.69 | loss  0.78 | perplexity     2.18\n",
      "|  1000/ 1800 batches | ms/batch 22.21 | loss  0.77 | perplexity     2.16\n",
      "|  1200/ 1800 batches | ms/batch 22.16 | loss  0.77 | perplexity     2.15\n",
      "|  1400/ 1800 batches | ms/batch 22.19 | loss  0.76 | perplexity     2.14\n",
      "|  1600/ 1800 batches | ms/batch 22.18 | loss  0.77 | perplexity     2.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 47.15s | test accuracy  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "Dataset à 9 bits : ('366704374+662259514=', '1028963888')\n",
      "Training 30 epochs on 9 bits\n",
      "-----------------------------------------------------------------------------------------\n",
      "| initialisation | test accuracy  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "|   200/ 1800 batches | ms/batch 27.05 | loss  2.17 | perplexity     8.76\n",
      "|   400/ 1800 batches | ms/batch 26.49 | loss  1.90 | perplexity     6.69\n",
      "|   600/ 1800 batches | ms/batch 26.42 | loss  1.81 | perplexity     6.09\n",
      "|   800/ 1800 batches | ms/batch 26.40 | loss  1.77 | perplexity     5.86\n",
      "|  1000/ 1800 batches | ms/batch 26.40 | loss  1.75 | perplexity     5.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# get training results\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnbits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bits\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m results[nbits] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     37\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m     40\u001b[0m     accuracy_v_epoch\u001b[38;5;241m.\u001b[39mappend(test_accuracy)\n",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m target_answers \u001b[38;5;241m=\u001b[39m target_answers\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output_answers, target_answers)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/LLM/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "list_nbits = [3,4,5,9]\n",
    "list_epochs = [5,10,30,30]\n",
    "results = {}\n",
    "\n",
    "for epochs, nbits in zip(list_epochs,list_nbits):\n",
    "    # create dataset\n",
    "    data = []\n",
    "    for _ in range(dataset_size):\n",
    "        data.append(sample_datapoint(number_bits=nbits))\n",
    "    data_train = data[: int(train_proportion * dataset_size)]\n",
    "    data_test = data[int(train_proportion * dataset_size):]\n",
    "    # reporting out\n",
    "    print(f\"Dataset à {nbits} bits : {data_train[0]}\")\n",
    "    # instantiate model\n",
    "    model = TransformerModel(ntoken = ntokens,\n",
    "                         ninp = 128,\n",
    "                         nhead = 16,\n",
    "                         nhid = 64,\n",
    "                         nlayers = 8)\n",
    "    model.to(device)\n",
    "    # get training results\n",
    "    print(f\"Training {epochs} epochs on {nbits} bits\")\n",
    "    results[nbits] = train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f49838",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "for r, accs in results.items():\n",
    "    ax.plot(accs, label=f\"{r}-bits\")\n",
    "    \n",
    "ax.set_title(f\"test accuracy vs epoch\")\n",
    "ax.set_xlabel(f\"epochs\")\n",
    "ax.set_ylabel(f\"accuracy\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qJ9IOZu8Xo4Y",
   "metadata": {
    "id": "qJ9IOZu8Xo4Y"
   },
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1213",
   "metadata": {},
   "source": [
    "This is just for fun..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yomPfirhXkLb",
   "metadata": {
    "id": "yomPfirhXkLb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 100\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def data_probing(size):\n",
    "    X = []\n",
    "    y = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        input = torch.tensor(tokenizer.encode(data[i][0])).view((-1, 1)).to(device)\n",
    "        _, output = model(input)\n",
    "        output = output[-1,:,:].flatten()\n",
    "        # determine whether there was a carry in the result:\n",
    "        carry = len(data[i][1]) > len(data[i][0]) / 2\n",
    "        X.append(output.cpu().detach().numpy())\n",
    "        y[i] = carry\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QGmfXVxkppfP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGmfXVxkppfP",
    "outputId": "6601c884-004f-40bb-8a1a-71995b17d860"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = data_probing(train_size)\n",
    "X_test, y_test = data_probing(test_size)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "reg = LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
